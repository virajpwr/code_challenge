INFO: 2022-09-27 11:14:19,962: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:14:19,991: X_train shape: (6436, 15)
INFO: 2022-09-27 11:14:19,991: X_test shape: (1609, 15)
INFO: 2022-09-27 11:14:19,991: Training the baseline model
INFO: 2022-09-27 11:14:19,998: Predicting on test data using baseline model
INFO: 2022-09-27 11:14:20,001: Training the XGB model
INFO: 2022-09-27 11:14:24,007: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:14:41,907: Predicting on test data using random forest
INFO: 2022-09-27 11:14:41,994: Creating plots
INFO: 2022-09-27 11:27:16,374: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:28:13,719: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:28:13,719: Splitting the data into train and test
INFO: 2022-09-27 11:28:13,753: X_train shape: (6436, 15)
INFO: 2022-09-27 11:28:13,753: X_test shape: (1609, 15)
INFO: 2022-09-27 11:28:13,753: Training the baseline model
INFO: 2022-09-27 11:28:13,753: Training the baseline model
INFO: 2022-09-27 11:28:13,755: Training the XGB model
INFO: 2022-09-27 11:28:13,755: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:28:13,818: Training the xgboost model
INFO: 2022-09-27 11:28:17,409: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:28:17,409: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:29:48,486: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:29:48,486: Splitting the data into train and test
INFO: 2022-09-27 11:29:48,518: X_train shape: (6436, 13)
INFO: 2022-09-27 11:29:48,518: X_test shape: (1609, 13)
INFO: 2022-09-27 11:29:48,518: Training the baseline model
INFO: 2022-09-27 11:29:48,518: Training the baseline model
INFO: 2022-09-27 11:29:48,520: Training the XGB model
INFO: 2022-09-27 11:29:48,520: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:29:48,580: Training the xgboost model
INFO: 2022-09-27 11:29:52,139: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:29:52,139: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:30:06,601: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:30:06,601: Lowest RMSE for random forest model: 0.721757732954195
INFO: 2022-09-27 11:30:06,602: Training the random forest model
INFO: 2022-09-27 11:30:06,602: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:30:10,420: Predicting on test data using baseline model
INFO: 2022-09-27 11:34:50,075: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:34:50,075: Splitting the data into train and test
INFO: 2022-09-27 11:34:50,113: X_train shape: (6436, 16)
INFO: 2022-09-27 11:34:50,113: X_test shape: (1609, 16)
INFO: 2022-09-27 11:34:50,113: Training the baseline model
INFO: 2022-09-27 11:34:50,113: Training the baseline model
INFO: 2022-09-27 11:34:50,115: Training the XGB model
INFO: 2022-09-27 11:34:50,115: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:34:50,179: Training the xgboost model
INFO: 2022-09-27 11:34:53,665: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:34:53,665: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:35:07,940: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:35:07,941: Lowest RMSE for random forest model: 0.7157636367359976
INFO: 2022-09-27 11:35:07,941: Training the random forest model
INFO: 2022-09-27 11:35:07,941: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:35:11,676: Predicting on test data using baseline model
INFO: 2022-09-27 11:35:11,676: predicting the target values for the test data using basline model
INFO: 2022-09-27 11:35:11,693: Predicting on test data using random forest
INFO: 2022-09-27 11:35:11,693: predicting the target values for the test data using random forest model
INFO: 2022-09-27 11:35:11,876: Creating plots
INFO: 2022-09-27 11:35:20,125: Saving the model
INFO: 2022-09-27 11:39:11,053: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:39:11,053: Splitting the data into train and test
INFO: 2022-09-27 11:39:11,086: X_train shape: (6436, 15)
INFO: 2022-09-27 11:39:11,086: X_test shape: (1609, 15)
INFO: 2022-09-27 11:39:11,086: Training the baseline model
INFO: 2022-09-27 11:39:11,086: Training the baseline model
INFO: 2022-09-27 11:39:11,088: Training the XGB model
INFO: 2022-09-27 11:39:11,088: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:39:11,142: Training the xgboost model
INFO: 2022-09-27 11:39:14,667: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:39:14,667: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:39:28,696: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:39:28,696: Lowest RMSE for random forest model: 0.7172239023680739
INFO: 2022-09-27 11:39:28,696: Training the random forest model
INFO: 2022-09-27 11:39:28,696: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:48:34,105: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:48:34,105: Splitting the data into train and test
INFO: 2022-09-27 11:48:34,138: X_train shape: (6436, 14)
INFO: 2022-09-27 11:48:34,138: X_test shape: (1609, 14)
INFO: 2022-09-27 11:48:34,138: Training the baseline model
INFO: 2022-09-27 11:48:34,138: Training the baseline model
INFO: 2022-09-27 11:48:34,140: Training the XGB model
INFO: 2022-09-27 11:48:34,140: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:48:34,195: Training the xgboost model
INFO: 2022-09-27 11:48:38,044: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:48:38,044: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:48:52,291: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:48:52,291: Lowest RMSE for random forest model: 0.7186957058667227
INFO: 2022-09-27 11:48:52,291: Training the random forest model
INFO: 2022-09-27 11:48:52,291: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:49:43,999: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:49:43,999: Splitting the data into train and test
INFO: 2022-09-27 11:49:44,033: X_train shape: (6436, 15)
INFO: 2022-09-27 11:49:44,033: X_test shape: (1609, 15)
INFO: 2022-09-27 11:49:44,033: Training the baseline model
INFO: 2022-09-27 11:49:44,033: Training the baseline model
INFO: 2022-09-27 11:49:44,035: Training the XGB model
INFO: 2022-09-27 11:49:44,035: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:49:44,099: Training the xgboost model
INFO: 2022-09-27 11:49:47,789: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:49:47,789: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:50:01,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:50:01,576: Lowest RMSE for random forest model: 0.7168344535093738
INFO: 2022-09-27 11:50:01,576: Training the random forest model
INFO: 2022-09-27 11:50:01,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:51:49,505: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:51:49,506: Splitting the data into train and test
INFO: 2022-09-27 11:51:49,536: X_train shape: (6436, 15)
INFO: 2022-09-27 11:51:49,536: X_test shape: (1609, 15)
INFO: 2022-09-27 11:51:49,536: Training the baseline model
INFO: 2022-09-27 11:51:49,536: Training the baseline model
INFO: 2022-09-27 11:51:49,538: Training the XGB model
INFO: 2022-09-27 11:51:49,538: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:51:49,601: Training the xgboost model
INFO: 2022-09-27 11:51:53,057: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:51:53,057: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:52:05,812: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:52:05,812: Lowest RMSE for random forest model: 0.7179779811938503
INFO: 2022-09-27 11:52:05,813: Training the random forest model
INFO: 2022-09-27 11:52:05,813: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:14:39,307: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:14:39,308: Splitting the data into train and test
INFO: 2022-09-27 12:14:39,338: X_train shape: (6436, 14)
INFO: 2022-09-27 12:14:39,338: X_test shape: (1609, 14)
INFO: 2022-09-27 12:14:39,338: Training the baseline model
INFO: 2022-09-27 12:14:39,338: Training the baseline model
INFO: 2022-09-27 12:14:39,345: Training the XGB model
INFO: 2022-09-27 12:14:39,345: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:14:39,467: Training the xgboost model
INFO: 2022-09-27 12:14:43,250: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:14:43,250: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:14:58,889: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:14:58,890: Lowest RMSE for random forest model: 0.7188828856607219
INFO: 2022-09-27 12:14:58,890: Training the random forest model
INFO: 2022-09-27 12:14:58,890: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:17:12,081: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:17:12,081: Splitting the data into train and test
INFO: 2022-09-27 12:17:12,116: X_train shape: (6436, 16)
INFO: 2022-09-27 12:17:12,117: X_test shape: (1609, 16)
INFO: 2022-09-27 12:17:12,117: Training the baseline model
INFO: 2022-09-27 12:17:12,117: Training the baseline model
INFO: 2022-09-27 12:17:12,118: Training the XGB model
INFO: 2022-09-27 12:17:12,118: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:17:12,185: Training the xgboost model
INFO: 2022-09-27 12:17:15,693: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:17:15,694: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:17:32,587: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:17:32,587: Lowest RMSE for random forest model: 0.7154237504120871
INFO: 2022-09-27 12:17:32,587: Training the random forest model
INFO: 2022-09-27 12:17:32,587: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:20:55,571: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:20:55,571: Splitting the data into train and test
INFO: 2022-09-27 12:20:55,602: X_train shape: (6436, 15)
INFO: 2022-09-27 12:20:55,602: X_test shape: (1609, 15)
INFO: 2022-09-27 12:20:55,602: Training the baseline model
INFO: 2022-09-27 12:20:55,602: Training the baseline model
INFO: 2022-09-27 12:20:55,604: Training the XGB model
INFO: 2022-09-27 12:20:55,604: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:20:55,667: Training the xgboost model
INFO: 2022-09-27 12:20:59,231: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:20:59,232: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:21:13,084: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:21:13,084: Lowest RMSE for random forest model: 0.7179845301281067
INFO: 2022-09-27 12:21:13,084: Training the random forest model
INFO: 2022-09-27 12:21:13,084: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:25:53,431: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:25:53,431: Splitting the data into train and test
INFO: 2022-09-27 12:25:53,462: X_train shape: (6436, 13)
INFO: 2022-09-27 12:25:53,462: X_test shape: (1609, 13)
INFO: 2022-09-27 12:25:53,462: Training the baseline model
INFO: 2022-09-27 12:25:53,462: Training the baseline model
INFO: 2022-09-27 12:25:53,464: Training the XGB model
INFO: 2022-09-27 12:25:53,464: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:25:53,521: Training the xgboost model
INFO: 2022-09-27 12:25:57,097: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:25:57,097: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:26:11,387: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:26:11,387: Lowest RMSE for random forest model: 0.7214268901059684
INFO: 2022-09-27 12:26:11,388: Training the random forest model
INFO: 2022-09-27 12:26:11,388: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:29,063: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:42:29,067: Splitting the data into train and test
INFO: 2022-09-27 12:42:29,104: X_train shape: (6436, 14)
INFO: 2022-09-27 12:42:29,105: X_test shape: (1609, 14)
INFO: 2022-09-27 12:42:29,105: Training the baseline model
INFO: 2022-09-27 12:42:29,105: Training the baseline model
INFO: 2022-09-27 12:42:29,111: Training the XGB model
INFO: 2022-09-27 12:42:29,111: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:42:29,229: Training the xgboost model
INFO: 2022-09-27 12:42:33,295: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:42:33,296: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:42:50,985: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:50,985: Lowest RMSE for random forest model: 0.7186080539946481
INFO: 2022-09-27 12:42:50,986: Training the random forest model
INFO: 2022-09-27 12:42:50,986: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:55,615: Saving the model
INFO: 2022-09-27 12:44:50,571: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:44:50,571: Splitting the data into train and test
INFO: 2022-09-27 12:44:50,602: X_train shape: (6436, 15)
INFO: 2022-09-27 12:44:50,602: X_test shape: (1609, 15)
INFO: 2022-09-27 12:44:50,602: Training the baseline model
INFO: 2022-09-27 12:44:50,602: Training the baseline model
INFO: 2022-09-27 12:44:50,604: Training the XGB model
INFO: 2022-09-27 12:44:50,604: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:44:50,667: Training the xgboost model
INFO: 2022-09-27 12:44:54,725: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:44:54,726: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:45:08,917: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:45:08,917: Lowest RMSE for random forest model: 0.7171727891747516
INFO: 2022-09-27 12:45:08,918: Training the random forest model
INFO: 2022-09-27 12:45:08,918: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:45:13,310: Saving the model
INFO: 2022-09-27 12:52:23,880: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:52:23,880: Splitting the data into train and test
INFO: 2022-09-27 12:52:23,917: X_train shape: (6436, 14)
INFO: 2022-09-27 12:52:23,917: X_test shape: (1609, 14)
INFO: 2022-09-27 12:52:23,917: Training the baseline model
INFO: 2022-09-27 12:52:23,917: Training the baseline model
INFO: 2022-09-27 12:52:23,923: Training the XGB model
INFO: 2022-09-27 12:52:23,923: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:52:24,047: Training the xgboost model
INFO: 2022-09-27 12:52:28,886: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:52:28,886: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:53:58,264: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:53:58,264: Lowest RMSE for random forest model: 0.7162789813446643
INFO: 2022-09-27 12:53:58,265: Training the random forest model
INFO: 2022-09-27 12:53:58,265: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:54:10,722: Saving the model
INFO: 2022-09-27 13:31:13,862: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 13:31:13,862: Splitting the data into train and test
INFO: 2022-09-27 13:31:13,905: X_train shape: (6436, 16)
INFO: 2022-09-27 13:31:13,905: X_test shape: (1609, 16)
INFO: 2022-09-27 13:31:13,905: Training the baseline model
INFO: 2022-09-27 13:31:13,905: Training the baseline model
INFO: 2022-09-27 13:31:13,910: Training the XGB model
INFO: 2022-09-27 13:31:13,910: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 13:31:14,039: Training the xgboost model
INFO: 2022-09-27 13:31:18,929: Hyper parameter tuning for random forest
INFO: 2022-09-27 13:31:18,929: Hyperparameter tuning for random forest model
INFO: 2022-09-27 13:33:06,213: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 13:33:06,214: Lowest RMSE for random forest model: 0.7138738205038734
INFO: 2022-09-27 13:33:06,217: Training the random forest model
INFO: 2022-09-27 13:33:06,217: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 13:33:22,624: Saving the model
INFO: 2022-09-27 14:00:10,338: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:00:10,338: Splitting the data into train and test
INFO: 2022-09-27 14:00:10,377: X_train shape: (6436, 16)
INFO: 2022-09-27 14:00:10,378: X_test shape: (1609, 16)
INFO: 2022-09-27 14:00:10,378: Training the baseline model
INFO: 2022-09-27 14:00:10,378: Training the baseline model
INFO: 2022-09-27 14:00:10,383: Training the XGB model
INFO: 2022-09-27 14:00:10,383: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:00:10,502: Training the xgboost model
INFO: 2022-09-27 14:00:15,792: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:00:15,793: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:01:25,631: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:01:25,631: Lowest RMSE for random forest model: 0.7143592831052938
INFO: 2022-09-27 14:01:25,632: Training the random forest model
INFO: 2022-09-27 14:01:25,632: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:01:38,833: Saving the model
INFO: 2022-09-27 14:29:06,707: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:29:06,711: Splitting the data into train and test
INFO: 2022-09-27 14:29:06,754: X_train shape: (6436, 15)
INFO: 2022-09-27 14:29:06,754: X_test shape: (1609, 15)
INFO: 2022-09-27 14:29:06,754: Training the baseline model
INFO: 2022-09-27 14:29:06,755: Training the baseline model
INFO: 2022-09-27 14:29:06,761: Training the XGB model
INFO: 2022-09-27 14:29:06,761: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:29:06,880: Training the xgboost model
INFO: 2022-09-27 14:29:11,249: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:29:11,250: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:30:32,943: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:30:32,943: Lowest RMSE for random forest model: 0.7154748707246532
INFO: 2022-09-27 14:30:32,944: Training the random forest model
INFO: 2022-09-27 14:30:32,944: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:30:43,758: Saving the model
INFO: 2022-09-27 14:32:14,922: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:32:14,922: Splitting the data into train and test
INFO: 2022-09-27 14:32:14,956: X_train shape: (6436, 15)
INFO: 2022-09-27 14:32:14,956: X_test shape: (1609, 15)
INFO: 2022-09-27 14:32:14,956: Training the baseline model
INFO: 2022-09-27 14:32:14,956: Training the baseline model
INFO: 2022-09-27 14:32:14,958: Training the XGB model
INFO: 2022-09-27 14:32:14,958: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:32:15,021: Training the xgboost model
INFO: 2022-09-27 14:32:19,419: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:32:19,420: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:33:35,121: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:33:35,121: Lowest RMSE for random forest model: 0.7175292909608055
INFO: 2022-09-27 14:33:35,122: Training the random forest model
INFO: 2022-09-27 14:33:35,122: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:33:39,869: Saving the model
INFO: 2022-09-27 14:49:58,369: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:49:58,369: Splitting the data into train and test
INFO: 2022-09-27 14:49:58,399: X_train shape: (6436, 14)
INFO: 2022-09-27 14:49:58,399: X_test shape: (1609, 14)
INFO: 2022-09-27 14:49:58,400: Training the baseline model
INFO: 2022-09-27 14:49:58,400: Training the baseline model
INFO: 2022-09-27 14:49:58,409: Training the XGB model
INFO: 2022-09-27 14:49:58,409: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:49:58,520: Training the xgboost model
INFO: 2022-09-27 14:50:02,984: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:50:02,984: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:51:33,566: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:51:33,567: Lowest RMSE for random forest model: 0.7173453473023734
INFO: 2022-09-27 14:51:33,570: Training the random forest model
INFO: 2022-09-27 14:51:33,570: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:51:46,001: Saving the model
INFO: 2022-09-27 15:02:25,659: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:02:25,659: Splitting the data into train and test
INFO: 2022-09-27 15:02:25,692: X_train shape: (6436, 21)
INFO: 2022-09-27 15:02:25,693: X_test shape: (1609, 21)
INFO: 2022-09-27 15:02:25,693: Training the baseline model
INFO: 2022-09-27 15:02:25,693: Training the baseline model
INFO: 2022-09-27 15:02:25,697: Training the XGB model
INFO: 2022-09-27 15:02:25,697: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:02:25,770: Training the xgboost model
INFO: 2022-09-27 15:02:29,494: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:02:29,495: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:03:18,853: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:03:18,853: Lowest RMSE for random forest model: 0.7128611966836764
INFO: 2022-09-27 15:03:18,853: Training the random forest model
INFO: 2022-09-27 15:03:18,853: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:03:22,201: Saving the model
INFO: 2022-09-27 15:13:07,702: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:13:07,703: Splitting the data into train and test
INFO: 2022-09-27 15:13:07,734: X_train shape: (6436, 20)
INFO: 2022-09-27 15:13:07,734: X_test shape: (1609, 20)
INFO: 2022-09-27 15:13:07,734: Training the baseline model
INFO: 2022-09-27 15:13:07,734: Training the baseline model
INFO: 2022-09-27 15:13:07,737: Training the XGB model
INFO: 2022-09-27 15:13:07,737: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:13:07,817: Training the xgboost model
INFO: 2022-09-27 15:13:11,086: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:13:11,086: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:14:28,249: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:14:28,251: Lowest RMSE for random forest model: 0.7118586182604604
INFO: 2022-09-27 15:14:28,259: Training the random forest model
INFO: 2022-09-27 15:14:28,259: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:14:38,948: Saving the model
INFO: 2022-09-27 15:49:39,432: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:49:39,432: Splitting the data into train and test
INFO: 2022-09-27 15:49:39,464: X_train shape: (6436, 20)
INFO: 2022-09-27 15:49:39,464: X_test shape: (1609, 20)
INFO: 2022-09-27 15:49:39,464: Training the baseline model
INFO: 2022-09-27 15:49:39,464: Training the baseline model
INFO: 2022-09-27 15:49:39,468: Training the XGB model
INFO: 2022-09-27 15:49:39,468: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:49:39,553: Training the xgboost model
INFO: 2022-09-27 15:49:43,048: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:49:43,048: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:50:44,909: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:50:44,909: Lowest RMSE for random forest model: 0.7145944838910999
INFO: 2022-09-27 15:50:44,910: Training the random forest model
INFO: 2022-09-27 15:50:44,910: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:50:49,017: Saving the model
INFO: 2022-09-27 20:32:23,447: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 20:32:23,450: Splitting the data into train and test
INFO: 2022-09-27 20:32:23,483: X_train shape: (6436, 22)
INFO: 2022-09-27 20:32:23,483: X_test shape: (1609, 22)
INFO: 2022-09-27 20:32:23,483: Training the baseline model
INFO: 2022-09-27 20:32:23,483: Training the baseline model
INFO: 2022-09-27 20:32:23,491: Training the XGB model
INFO: 2022-09-27 20:32:23,491: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 20:32:23,578: Training the xgboost model
INFO: 2022-09-27 20:32:27,982: Hyper parameter tuning for random forest
INFO: 2022-09-27 20:32:27,982: Hyperparameter tuning for random forest model
INFO: 2022-09-27 20:33:44,277: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 20:33:44,277: Lowest RMSE for random forest model: 0.7100508829052756
INFO: 2022-09-27 20:33:44,278: Training the random forest model
INFO: 2022-09-27 20:33:44,278: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 20:33:54,366: Saving the model
INFO: 2022-09-28 07:54:44,538: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 07:54:44,539: Splitting the data into train and test
INFO: 2022-09-28 07:54:44,581: X_train shape: (6436, 22)
INFO: 2022-09-28 07:54:44,582: X_test shape: (1609, 22)
INFO: 2022-09-28 07:54:44,582: Training the XGB model
INFO: 2022-09-28 07:54:44,582: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 07:54:44,725: Training the xgboost model
INFO: 2022-09-28 07:54:49,938: Hyper parameter tuning for random forest
INFO: 2022-09-28 07:54:49,938: Hyperparameter tuning for random forest model
ERROR: 2022-09-28 07:54:52,044: exception calling callback for <Future at 0x28a84b169a0 state=finished raised TerminatedWorkerError>
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.

INFO: 2022-09-28 08:38:12,541: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 08:38:12,542: Splitting the data into train and test
INFO: 2022-09-28 08:38:12,594: X_train shape: (6436, 23)
INFO: 2022-09-28 08:38:12,594: X_test shape: (1609, 23)
INFO: 2022-09-28 08:38:12,594: Training the baseline model
INFO: 2022-09-28 08:38:12,594: Training the baseline model
INFO: 2022-09-28 08:38:12,600: Training the XGB model
INFO: 2022-09-28 08:38:12,601: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 08:38:12,768: Training the xgboost model
INFO: 2022-09-28 08:38:17,502: Hyper parameter tuning for random forest
INFO: 2022-09-28 08:38:17,502: Hyperparameter tuning for random forest model
INFO: 2022-09-28 08:39:31,766: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 08:39:31,766: Lowest RMSE for random forest model: 0.7124156090349977
INFO: 2022-09-28 08:39:31,769: Training the random forest model
INFO: 2022-09-28 08:39:31,769: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 08:39:42,172: Saving the model
INFO: 2022-09-28 09:01:25,205: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 09:01:25,206: Splitting the data into train and test
INFO: 2022-09-28 09:01:25,250: X_train shape: (6436, 21)
INFO: 2022-09-28 09:01:25,250: X_test shape: (1609, 21)
INFO: 2022-09-28 09:01:25,250: Training the baseline model
INFO: 2022-09-28 09:01:25,251: Training the baseline model
INFO: 2022-09-28 09:01:25,258: Training the XGB model
INFO: 2022-09-28 09:01:25,258: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 09:01:25,431: Training the xgboost model
INFO: 2022-09-28 09:01:31,592: Hyper parameter tuning for random forest
INFO: 2022-09-28 09:01:31,593: Hyperparameter tuning for random forest model
INFO: 2022-09-28 09:02:51,992: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 09:02:51,994: Lowest RMSE for random forest model: 0.7142888692400011
INFO: 2022-09-28 09:02:52,001: Training the random forest model
INFO: 2022-09-28 09:02:52,001: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 09:03:02,977: Saving the model
INFO: 2022-09-28 09:20:07,923: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 09:21:54,206: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 19:57:24,835: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 19:57:24,835: Splitting the data into train and test
INFO: 2022-09-28 19:57:24,863: X_train shape: (6436, 17)
INFO: 2022-09-28 19:57:24,863: X_test shape: (1609, 17)
INFO: 2022-09-28 19:57:24,864: Training the baseline model
INFO: 2022-09-28 19:57:24,864: Training the baseline model
INFO: 2022-09-28 19:57:24,869: Training the XGB model
INFO: 2022-09-28 19:57:24,869: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 19:57:25,010: Training the xgboost model
INFO: 2022-09-28 19:57:29,539: Hyper parameter tuning for random forest
INFO: 2022-09-28 19:57:29,540: Hyperparameter tuning for random forest model
INFO: 2022-09-28 19:58:29,243: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 19:58:29,243: Lowest RMSE for random forest model: 0.7290249345760205
INFO: 2022-09-28 19:58:29,244: Training the random forest model
INFO: 2022-09-28 19:58:29,244: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 19:58:39,417: Saving the model
INFO: 2022-09-28 20:14:05,703: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:14:05,706: Splitting the data into train and test
INFO: 2022-09-28 20:14:05,731: X_train shape: (6436, 17)
INFO: 2022-09-28 20:14:05,731: X_test shape: (1609, 17)
INFO: 2022-09-28 20:14:05,731: Training the baseline model
INFO: 2022-09-28 20:14:05,731: Training the baseline model
INFO: 2022-09-28 20:14:05,735: Training the XGB model
INFO: 2022-09-28 20:14:05,735: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:14:05,846: Training the xgboost model
INFO: 2022-09-28 20:14:10,042: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:14:10,042: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:14:55,229: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:14:55,229: Lowest RMSE for random forest model: 0.734937581056365
INFO: 2022-09-28 20:14:55,230: Training the random forest model
INFO: 2022-09-28 20:14:55,230: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:14:58,903: Saving the model
INFO: 2022-09-28 20:29:34,059: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:29:34,080: Splitting the data into train and test
INFO: 2022-09-28 20:29:34,115: X_train shape: (6436, 17)
INFO: 2022-09-28 20:29:34,115: X_test shape: (1609, 17)
INFO: 2022-09-28 20:29:34,115: Training the baseline model
INFO: 2022-09-28 20:29:34,115: Training the baseline model
INFO: 2022-09-28 20:29:34,120: Training the XGB model
INFO: 2022-09-28 20:29:34,120: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:29:34,281: Training the xgboost model
INFO: 2022-09-28 20:29:40,998: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:29:40,999: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:31:13,149: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:31:13,152: Lowest RMSE for random forest model: 0.7342929450312508
INFO: 2022-09-28 20:31:13,167: Training the random forest model
INFO: 2022-09-28 20:31:13,167: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:31:22,582: Saving the model
INFO: 2022-09-28 20:33:57,717: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:33:57,717: Splitting the data into train and test
INFO: 2022-09-28 20:33:57,750: X_train shape: (6436, 17)
INFO: 2022-09-28 20:33:57,750: X_test shape: (1609, 17)
INFO: 2022-09-28 20:33:57,750: Training the baseline model
INFO: 2022-09-28 20:33:57,750: Training the baseline model
INFO: 2022-09-28 20:33:57,753: Training the XGB model
INFO: 2022-09-28 20:33:57,753: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:33:57,864: Training the xgboost model
INFO: 2022-09-28 20:34:02,828: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:34:02,828: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:35:15,819: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-28 20:35:15,820: Lowest RMSE for random forest model: 0.7387118314251307
INFO: 2022-09-28 20:35:15,821: Training the random forest model
INFO: 2022-09-28 20:35:15,821: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-28 20:35:27,343: Saving the model
INFO: 2022-09-28 20:38:21,116: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:38:21,116: Splitting the data into train and test
INFO: 2022-09-28 20:38:21,143: X_train shape: (6436, 17)
INFO: 2022-09-28 20:38:21,143: X_test shape: (1609, 17)
INFO: 2022-09-28 20:38:21,144: Training the baseline model
INFO: 2022-09-28 20:38:21,144: Training the baseline model
INFO: 2022-09-28 20:38:21,147: Training the XGB model
INFO: 2022-09-28 20:38:21,147: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:38:21,229: Training the xgboost model
INFO: 2022-09-28 20:38:25,565: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:38:25,565: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:39:27,907: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:39:27,907: Lowest RMSE for random forest model: 0.7347712969533984
INFO: 2022-09-28 20:39:27,907: Training the random forest model
INFO: 2022-09-28 20:39:27,907: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:39:36,720: Saving the model
INFO: 2022-09-28 21:06:15,774: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 21:06:15,774: Splitting the data into train and test
INFO: 2022-09-28 21:06:15,801: X_train shape: (6436, 17)
INFO: 2022-09-28 21:06:15,801: X_test shape: (1609, 17)
INFO: 2022-09-28 21:06:15,801: Training the baseline model
INFO: 2022-09-28 21:06:15,801: Training the baseline model
INFO: 2022-09-28 21:06:15,807: Training the XGB model
INFO: 2022-09-28 21:06:15,807: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 21:06:15,924: Training the xgboost model
INFO: 2022-09-28 21:06:20,525: Hyper parameter tuning for random forest
INFO: 2022-09-28 21:06:20,525: Hyperparameter tuning for random forest model
INFO: 2022-09-28 21:07:27,262: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 21:07:27,262: Lowest RMSE for random forest model: 0.7338636554002407
INFO: 2022-09-28 21:07:27,263: Training the random forest model
INFO: 2022-09-28 21:07:27,263: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 21:07:31,476: Saving the model
INFO: 2022-09-29 08:56:24,704: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-29 08:56:24,705: Splitting the data into train and test
INFO: 2022-09-29 08:56:24,741: X_train shape: (6436, 19)
INFO: 2022-09-29 08:56:24,741: X_test shape: (1609, 19)
INFO: 2022-09-29 08:56:24,741: Training the baseline model
INFO: 2022-09-29 08:56:24,741: Training the baseline model
INFO: 2022-09-29 08:56:24,748: Training the XGB model
INFO: 2022-09-29 08:56:24,748: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-29 08:56:24,917: Training the xgboost model
INFO: 2022-09-29 08:56:30,128: Hyper parameter tuning for random forest
INFO: 2022-09-29 08:56:30,129: Hyperparameter tuning for random forest model
INFO: 2022-09-29 08:57:28,336: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-29 08:57:28,336: Lowest RMSE for random forest model: 0.7342271901376728
INFO: 2022-09-29 08:57:28,337: Training the random forest model
INFO: 2022-09-29 08:57:28,337: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-29 08:57:32,051: Saving the model
INFO: 2022-09-30 00:11:52,914: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:12:43,362: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:12:43,363: Hyperparameter tuning for xgboost model
INFO: 2022-09-30 00:13:43,034: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:13:43,034: Hyperparameter tuning for xgboost model
INFO: 2022-09-30 00:14:17,371: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:14:17,371: Hyperparameter tuning for xgboost model
ERROR: 2022-09-30 00:14:21,854: exception calling callback for <Future at 0x22c1c327a90 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 407, in _process_worker
    call_item = call_queue.get(block=True, timeout=timeout)
  File "c:\python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\__init__.py", line 6, in <module>
    from .core import (
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 231, in <module>
    _LIB = _load_lib()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 184, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (xgboost.dll) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['[WinError 1455] The paging file is too small for this operation to complete']

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-09-30 00:15:37,366: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:15:37,366: Hyperparameter tuning for xgboost model
ERROR: 2022-09-30 00:15:41,579: exception calling callback for <Future at 0x2df3cfd5a60 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 407, in _process_worker
    call_item = call_queue.get(block=True, timeout=timeout)
  File "c:\python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\__init__.py", line 6, in <module>
    from .core import (
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 231, in <module>
    _LIB = _load_lib()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 184, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (xgboost.dll) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['[WinError 1455] The paging file is too small for this operation to complete']

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-09-30 00:17:36,756: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:17:36,756: Splitting the data into train and test
INFO: 2022-09-30 00:17:36,790: X_train shape: (6436, 19)
INFO: 2022-09-30 00:17:36,790: X_test shape: (1609, 19)
INFO: 2022-09-30 00:17:36,790: Training the baseline model
INFO: 2022-09-30 00:17:36,790: Training the baseline model
INFO: 2022-09-30 00:17:36,796: Training the XGB model
INFO: 2022-09-30 00:17:36,796: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:17:36,938: Training the xgboost model
INFO: 2022-09-30 00:17:42,508: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:17:42,509: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:18:21,095: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:18:21,095: Lowest RMSE for random forest model: 0.747074571237682
INFO: 2022-09-30 00:18:21,096: Training the random forest model
INFO: 2022-09-30 00:18:21,096: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:18:24,373: Saving the model
INFO: 2022-09-30 00:19:00,275: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:19:00,275: Splitting the data into train and test
INFO: 2022-09-30 00:19:00,306: X_train shape: (6436, 19)
INFO: 2022-09-30 00:19:00,307: X_test shape: (1609, 19)
INFO: 2022-09-30 00:19:00,307: Training the baseline model
INFO: 2022-09-30 00:19:00,307: Training the baseline model
INFO: 2022-09-30 00:19:00,310: Training the XGB model
INFO: 2022-09-30 00:19:00,310: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:19:00,409: Training the xgboost model
INFO: 2022-09-30 00:19:05,470: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:19:05,471: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:19:51,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-30 00:19:51,576: Lowest RMSE for random forest model: 0.7353151789339766
INFO: 2022-09-30 00:19:51,577: Training the random forest model
INFO: 2022-09-30 00:19:51,577: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-30 00:19:55,613: Saving the model
INFO: 2022-09-30 00:53:27,642: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:53:27,643: Splitting the data into train and test
INFO: 2022-09-30 00:53:27,689: X_train shape: (6436, 19)
INFO: 2022-09-30 00:53:27,689: X_test shape: (1609, 19)
INFO: 2022-09-30 00:53:27,689: Training the baseline model
INFO: 2022-09-30 00:53:27,690: Training the baseline model
INFO: 2022-09-30 00:53:27,696: Training the XGB model
INFO: 2022-09-30 00:53:27,696: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:53:27,834: Training the xgboost model
INFO: 2022-09-30 00:53:33,797: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:53:33,797: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:54:40,963: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:54:40,964: Lowest RMSE for random forest model: 0.7468120524712232
INFO: 2022-09-30 00:54:40,964: Training the random forest model
INFO: 2022-09-30 00:54:40,964: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:54:52,912: Saving the model
INFO: 2022-09-30 01:20:14,811: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 01:20:14,811: Splitting the data into train and test
INFO: 2022-09-30 01:20:14,841: X_train shape: (6436, 19)
INFO: 2022-09-30 01:20:14,841: X_test shape: (1609, 19)
INFO: 2022-09-30 01:20:14,841: Training the baseline model
INFO: 2022-09-30 01:20:14,842: Training the baseline model
INFO: 2022-09-30 01:20:14,844: Training the XGB model
INFO: 2022-09-30 01:20:14,844: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 01:20:14,949: Training the xgboost model
INFO: 2022-09-30 01:20:20,336: Hyper parameter tuning for random forest
INFO: 2022-09-30 01:20:20,336: Hyperparameter tuning for random forest model
INFO: 2022-09-30 01:21:02,326: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-09-30 01:21:02,326: Lowest RMSE for random forest model: 0.7246445986076976
INFO: 2022-09-30 01:21:02,327: Training the random forest model
INFO: 2022-09-30 01:21:02,327: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-09-30 01:21:03,218: Saving the model
INFO: 2022-09-30 01:22:13,331: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 01:22:13,332: Splitting the data into train and test
INFO: 2022-09-30 01:22:13,370: X_train shape: (6436, 19)
INFO: 2022-09-30 01:22:13,370: X_test shape: (1609, 19)
INFO: 2022-09-30 01:22:13,370: Training the baseline model
INFO: 2022-09-30 01:22:13,370: Training the baseline model
INFO: 2022-09-30 01:22:13,373: Training the XGB model
INFO: 2022-09-30 01:22:13,374: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 01:22:13,487: Training the xgboost model
INFO: 2022-09-30 01:22:19,001: Hyper parameter tuning for random forest
INFO: 2022-09-30 01:22:19,001: Hyperparameter tuning for random forest model
INFO: 2022-09-30 01:23:16,771: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 01:23:16,771: Lowest RMSE for random forest model: 0.7244819633075281
INFO: 2022-09-30 01:23:16,771: Training the random forest model
INFO: 2022-09-30 01:23:16,772: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 01:23:21,715: Saving the model
INFO: 2022-09-30 09:14:38,222: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 09:14:38,222: Splitting the data into train and test
INFO: 2022-09-30 09:14:38,266: X_train shape: (6436, 19)
INFO: 2022-09-30 09:14:38,267: X_test shape: (1609, 19)
INFO: 2022-09-30 09:14:38,267: Training the baseline model
INFO: 2022-09-30 09:14:38,267: Training the baseline model
INFO: 2022-09-30 09:14:38,279: Training the XGB model
INFO: 2022-09-30 09:14:38,279: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 09:14:38,704: Training the xgboost model
INFO: 2022-09-30 09:14:46,208: Hyper parameter tuning for random forest
INFO: 2022-09-30 09:14:46,208: Hyperparameter tuning for random forest model
INFO: 2022-09-30 09:16:02,029: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 09:16:02,029: Lowest RMSE for random forest model: 0.7277483264706763
INFO: 2022-09-30 09:16:02,030: Training the random forest model
INFO: 2022-09-30 09:16:02,030: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 09:16:09,887: Saving the model
INFO: 2022-09-30 11:06:12,899: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 11:06:12,902: Splitting the data into train and test
INFO: 2022-09-30 11:06:12,932: X_train shape: (6436, 17)
INFO: 2022-09-30 11:06:12,932: X_test shape: (1609, 17)
INFO: 2022-09-30 11:06:12,932: Training the baseline model
INFO: 2022-09-30 11:06:12,932: Training the baseline model
INFO: 2022-09-30 11:06:12,938: Training the XGB model
INFO: 2022-09-30 11:06:12,938: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 11:06:13,074: Training the xgboost model
INFO: 2022-09-30 11:06:19,222: Hyper parameter tuning for random forest
INFO: 2022-09-30 11:06:19,222: Hyperparameter tuning for random forest model
INFO: 2022-09-30 11:07:15,515: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 30, 'bootstrap': True}
INFO: 2022-09-30 11:07:15,515: Lowest RMSE for random forest model: 0.730701941404418
INFO: 2022-09-30 11:07:15,516: Training the random forest model
INFO: 2022-09-30 11:07:15,516: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 30, 'bootstrap': True}
INFO: 2022-09-30 11:07:21,471: Saving the model
INFO: 2022-09-30 11:36:05,972: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 11:36:05,973: Splitting the data into train and test
INFO: 2022-09-30 11:36:06,007: X_train shape: (6436, 17)
INFO: 2022-09-30 11:36:06,007: X_test shape: (1609, 17)
INFO: 2022-09-30 11:36:06,007: Training the baseline model
INFO: 2022-09-30 11:36:06,007: Training the baseline model
INFO: 2022-09-30 11:36:06,012: Training the XGB model
INFO: 2022-09-30 11:36:06,013: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 11:36:06,097: Training the xgboost model
INFO: 2022-09-30 11:36:11,325: Hyper parameter tuning for random forest
INFO: 2022-09-30 11:36:11,325: Hyperparameter tuning for random forest model
INFO: 2022-09-30 11:37:27,964: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-09-30 11:37:27,965: Lowest RMSE for random forest model: 0.7171871726894145
INFO: 2022-09-30 11:37:27,965: Training the random forest model
INFO: 2022-09-30 11:37:27,965: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-09-30 11:37:39,369: Saving the model
INFO: 2022-09-30 13:46:05,766: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 13:46:05,767: Splitting the data into train and test
INFO: 2022-09-30 13:46:05,795: X_train shape: (6436, 17)
INFO: 2022-09-30 13:46:05,795: X_test shape: (1609, 17)
INFO: 2022-09-30 13:46:05,796: Training the baseline model
INFO: 2022-09-30 13:46:05,796: Training the baseline model
INFO: 2022-09-30 13:46:05,800: Training the XGB model
INFO: 2022-09-30 13:46:05,800: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 13:46:05,945: Training the xgboost model
INFO: 2022-09-30 13:46:10,847: Hyper parameter tuning for random forest
INFO: 2022-09-30 13:46:10,847: Hyperparameter tuning for random forest model
INFO: 2022-09-30 13:47:36,114: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-09-30 13:47:36,114: Lowest RMSE for random forest model: 0.7238512860742443
INFO: 2022-09-30 13:47:36,116: Training the random forest model
INFO: 2022-09-30 13:47:36,116: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-09-30 13:47:46,290: Saving the model
INFO: 2022-10-01 13:06:16,931: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-01 13:06:16,932: Splitting the data into train and test
INFO: 2022-10-01 13:06:16,958: X_train shape: (6436, 17)
INFO: 2022-10-01 13:06:16,958: X_test shape: (1609, 17)
INFO: 2022-10-01 13:06:16,958: Training the baseline model
INFO: 2022-10-01 13:06:16,958: Training the baseline model
INFO: 2022-10-01 13:06:16,964: Training the XGB model
INFO: 2022-10-01 13:06:16,964: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-01 13:06:17,073: Training the xgboost model
INFO: 2022-10-01 13:06:21,222: Hyper parameter tuning for random forest
INFO: 2022-10-01 13:06:21,222: Hyperparameter tuning for random forest model
INFO: 2022-10-01 13:07:17,063: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-01 13:07:17,063: Lowest RMSE for random forest model: 0.7242812981300921
INFO: 2022-10-01 13:07:17,063: Training the random forest model
INFO: 2022-10-01 13:07:17,063: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-01 13:07:20,359: Saving the model
