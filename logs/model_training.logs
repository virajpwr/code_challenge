INFO: 2022-09-27 11:14:19,962: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:14:19,991: X_train shape: (6436, 15)
INFO: 2022-09-27 11:14:19,991: X_test shape: (1609, 15)
INFO: 2022-09-27 11:14:19,991: Training the baseline model
INFO: 2022-09-27 11:14:19,998: Predicting on test data using baseline model
INFO: 2022-09-27 11:14:20,001: Training the XGB model
INFO: 2022-09-27 11:14:24,007: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:14:41,907: Predicting on test data using random forest
INFO: 2022-09-27 11:14:41,994: Creating plots
INFO: 2022-09-27 11:27:16,374: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:28:13,719: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:28:13,719: Splitting the data into train and test
INFO: 2022-09-27 11:28:13,753: X_train shape: (6436, 15)
INFO: 2022-09-27 11:28:13,753: X_test shape: (1609, 15)
INFO: 2022-09-27 11:28:13,753: Training the baseline model
INFO: 2022-09-27 11:28:13,753: Training the baseline model
INFO: 2022-09-27 11:28:13,755: Training the XGB model
INFO: 2022-09-27 11:28:13,755: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:28:13,818: Training the xgboost model
INFO: 2022-09-27 11:28:17,409: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:28:17,409: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:29:48,486: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:29:48,486: Splitting the data into train and test
INFO: 2022-09-27 11:29:48,518: X_train shape: (6436, 13)
INFO: 2022-09-27 11:29:48,518: X_test shape: (1609, 13)
INFO: 2022-09-27 11:29:48,518: Training the baseline model
INFO: 2022-09-27 11:29:48,518: Training the baseline model
INFO: 2022-09-27 11:29:48,520: Training the XGB model
INFO: 2022-09-27 11:29:48,520: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:29:48,580: Training the xgboost model
INFO: 2022-09-27 11:29:52,139: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:29:52,139: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:30:06,601: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:30:06,601: Lowest RMSE for random forest model: 0.721757732954195
INFO: 2022-09-27 11:30:06,602: Training the random forest model
INFO: 2022-09-27 11:30:06,602: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:30:10,420: Predicting on test data using baseline model
INFO: 2022-09-27 11:34:50,075: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:34:50,075: Splitting the data into train and test
INFO: 2022-09-27 11:34:50,113: X_train shape: (6436, 16)
INFO: 2022-09-27 11:34:50,113: X_test shape: (1609, 16)
INFO: 2022-09-27 11:34:50,113: Training the baseline model
INFO: 2022-09-27 11:34:50,113: Training the baseline model
INFO: 2022-09-27 11:34:50,115: Training the XGB model
INFO: 2022-09-27 11:34:50,115: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:34:50,179: Training the xgboost model
INFO: 2022-09-27 11:34:53,665: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:34:53,665: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:35:07,940: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:35:07,941: Lowest RMSE for random forest model: 0.7157636367359976
INFO: 2022-09-27 11:35:07,941: Training the random forest model
INFO: 2022-09-27 11:35:07,941: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:35:11,676: Predicting on test data using baseline model
INFO: 2022-09-27 11:35:11,676: predicting the target values for the test data using basline model
INFO: 2022-09-27 11:35:11,693: Predicting on test data using random forest
INFO: 2022-09-27 11:35:11,693: predicting the target values for the test data using random forest model
INFO: 2022-09-27 11:35:11,876: Creating plots
INFO: 2022-09-27 11:35:20,125: Saving the model
INFO: 2022-09-27 11:39:11,053: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:39:11,053: Splitting the data into train and test
INFO: 2022-09-27 11:39:11,086: X_train shape: (6436, 15)
INFO: 2022-09-27 11:39:11,086: X_test shape: (1609, 15)
INFO: 2022-09-27 11:39:11,086: Training the baseline model
INFO: 2022-09-27 11:39:11,086: Training the baseline model
INFO: 2022-09-27 11:39:11,088: Training the XGB model
INFO: 2022-09-27 11:39:11,088: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:39:11,142: Training the xgboost model
INFO: 2022-09-27 11:39:14,667: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:39:14,667: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:39:28,696: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:39:28,696: Lowest RMSE for random forest model: 0.7172239023680739
INFO: 2022-09-27 11:39:28,696: Training the random forest model
INFO: 2022-09-27 11:39:28,696: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:48:34,105: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:48:34,105: Splitting the data into train and test
INFO: 2022-09-27 11:48:34,138: X_train shape: (6436, 14)
INFO: 2022-09-27 11:48:34,138: X_test shape: (1609, 14)
INFO: 2022-09-27 11:48:34,138: Training the baseline model
INFO: 2022-09-27 11:48:34,138: Training the baseline model
INFO: 2022-09-27 11:48:34,140: Training the XGB model
INFO: 2022-09-27 11:48:34,140: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:48:34,195: Training the xgboost model
INFO: 2022-09-27 11:48:38,044: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:48:38,044: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:48:52,291: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:48:52,291: Lowest RMSE for random forest model: 0.7186957058667227
INFO: 2022-09-27 11:48:52,291: Training the random forest model
INFO: 2022-09-27 11:48:52,291: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:49:43,999: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:49:43,999: Splitting the data into train and test
INFO: 2022-09-27 11:49:44,033: X_train shape: (6436, 15)
INFO: 2022-09-27 11:49:44,033: X_test shape: (1609, 15)
INFO: 2022-09-27 11:49:44,033: Training the baseline model
INFO: 2022-09-27 11:49:44,033: Training the baseline model
INFO: 2022-09-27 11:49:44,035: Training the XGB model
INFO: 2022-09-27 11:49:44,035: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:49:44,099: Training the xgboost model
INFO: 2022-09-27 11:49:47,789: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:49:47,789: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:50:01,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:50:01,576: Lowest RMSE for random forest model: 0.7168344535093738
INFO: 2022-09-27 11:50:01,576: Training the random forest model
INFO: 2022-09-27 11:50:01,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:51:49,505: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 11:51:49,506: Splitting the data into train and test
INFO: 2022-09-27 11:51:49,536: X_train shape: (6436, 15)
INFO: 2022-09-27 11:51:49,536: X_test shape: (1609, 15)
INFO: 2022-09-27 11:51:49,536: Training the baseline model
INFO: 2022-09-27 11:51:49,536: Training the baseline model
INFO: 2022-09-27 11:51:49,538: Training the XGB model
INFO: 2022-09-27 11:51:49,538: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 11:51:49,601: Training the xgboost model
INFO: 2022-09-27 11:51:53,057: Hyper parameter tuning for random forest
INFO: 2022-09-27 11:51:53,057: Hyperparameter tuning for random forest model
INFO: 2022-09-27 11:52:05,812: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 11:52:05,812: Lowest RMSE for random forest model: 0.7179779811938503
INFO: 2022-09-27 11:52:05,813: Training the random forest model
INFO: 2022-09-27 11:52:05,813: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:14:39,307: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:14:39,308: Splitting the data into train and test
INFO: 2022-09-27 12:14:39,338: X_train shape: (6436, 14)
INFO: 2022-09-27 12:14:39,338: X_test shape: (1609, 14)
INFO: 2022-09-27 12:14:39,338: Training the baseline model
INFO: 2022-09-27 12:14:39,338: Training the baseline model
INFO: 2022-09-27 12:14:39,345: Training the XGB model
INFO: 2022-09-27 12:14:39,345: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:14:39,467: Training the xgboost model
INFO: 2022-09-27 12:14:43,250: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:14:43,250: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:14:58,889: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:14:58,890: Lowest RMSE for random forest model: 0.7188828856607219
INFO: 2022-09-27 12:14:58,890: Training the random forest model
INFO: 2022-09-27 12:14:58,890: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:17:12,081: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:17:12,081: Splitting the data into train and test
INFO: 2022-09-27 12:17:12,116: X_train shape: (6436, 16)
INFO: 2022-09-27 12:17:12,117: X_test shape: (1609, 16)
INFO: 2022-09-27 12:17:12,117: Training the baseline model
INFO: 2022-09-27 12:17:12,117: Training the baseline model
INFO: 2022-09-27 12:17:12,118: Training the XGB model
INFO: 2022-09-27 12:17:12,118: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:17:12,185: Training the xgboost model
INFO: 2022-09-27 12:17:15,693: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:17:15,694: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:17:32,587: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:17:32,587: Lowest RMSE for random forest model: 0.7154237504120871
INFO: 2022-09-27 12:17:32,587: Training the random forest model
INFO: 2022-09-27 12:17:32,587: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:20:55,571: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:20:55,571: Splitting the data into train and test
INFO: 2022-09-27 12:20:55,602: X_train shape: (6436, 15)
INFO: 2022-09-27 12:20:55,602: X_test shape: (1609, 15)
INFO: 2022-09-27 12:20:55,602: Training the baseline model
INFO: 2022-09-27 12:20:55,602: Training the baseline model
INFO: 2022-09-27 12:20:55,604: Training the XGB model
INFO: 2022-09-27 12:20:55,604: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:20:55,667: Training the xgboost model
INFO: 2022-09-27 12:20:59,231: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:20:59,232: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:21:13,084: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:21:13,084: Lowest RMSE for random forest model: 0.7179845301281067
INFO: 2022-09-27 12:21:13,084: Training the random forest model
INFO: 2022-09-27 12:21:13,084: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:25:53,431: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:25:53,431: Splitting the data into train and test
INFO: 2022-09-27 12:25:53,462: X_train shape: (6436, 13)
INFO: 2022-09-27 12:25:53,462: X_test shape: (1609, 13)
INFO: 2022-09-27 12:25:53,462: Training the baseline model
INFO: 2022-09-27 12:25:53,462: Training the baseline model
INFO: 2022-09-27 12:25:53,464: Training the XGB model
INFO: 2022-09-27 12:25:53,464: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:25:53,521: Training the xgboost model
INFO: 2022-09-27 12:25:57,097: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:25:57,097: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:26:11,387: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:26:11,387: Lowest RMSE for random forest model: 0.7214268901059684
INFO: 2022-09-27 12:26:11,388: Training the random forest model
INFO: 2022-09-27 12:26:11,388: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:29,063: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:42:29,067: Splitting the data into train and test
INFO: 2022-09-27 12:42:29,104: X_train shape: (6436, 14)
INFO: 2022-09-27 12:42:29,105: X_test shape: (1609, 14)
INFO: 2022-09-27 12:42:29,105: Training the baseline model
INFO: 2022-09-27 12:42:29,105: Training the baseline model
INFO: 2022-09-27 12:42:29,111: Training the XGB model
INFO: 2022-09-27 12:42:29,111: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:42:29,229: Training the xgboost model
INFO: 2022-09-27 12:42:33,295: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:42:33,296: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:42:50,985: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:50,985: Lowest RMSE for random forest model: 0.7186080539946481
INFO: 2022-09-27 12:42:50,986: Training the random forest model
INFO: 2022-09-27 12:42:50,986: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:42:55,615: Saving the model
INFO: 2022-09-27 12:44:50,571: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:44:50,571: Splitting the data into train and test
INFO: 2022-09-27 12:44:50,602: X_train shape: (6436, 15)
INFO: 2022-09-27 12:44:50,602: X_test shape: (1609, 15)
INFO: 2022-09-27 12:44:50,602: Training the baseline model
INFO: 2022-09-27 12:44:50,602: Training the baseline model
INFO: 2022-09-27 12:44:50,604: Training the XGB model
INFO: 2022-09-27 12:44:50,604: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:44:50,667: Training the xgboost model
INFO: 2022-09-27 12:44:54,725: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:44:54,726: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:45:08,917: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:45:08,917: Lowest RMSE for random forest model: 0.7171727891747516
INFO: 2022-09-27 12:45:08,918: Training the random forest model
INFO: 2022-09-27 12:45:08,918: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:45:13,310: Saving the model
INFO: 2022-09-27 12:52:23,880: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 12:52:23,880: Splitting the data into train and test
INFO: 2022-09-27 12:52:23,917: X_train shape: (6436, 14)
INFO: 2022-09-27 12:52:23,917: X_test shape: (1609, 14)
INFO: 2022-09-27 12:52:23,917: Training the baseline model
INFO: 2022-09-27 12:52:23,917: Training the baseline model
INFO: 2022-09-27 12:52:23,923: Training the XGB model
INFO: 2022-09-27 12:52:23,923: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 12:52:24,047: Training the xgboost model
INFO: 2022-09-27 12:52:28,886: Hyper parameter tuning for random forest
INFO: 2022-09-27 12:52:28,886: Hyperparameter tuning for random forest model
INFO: 2022-09-27 12:53:58,264: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:53:58,264: Lowest RMSE for random forest model: 0.7162789813446643
INFO: 2022-09-27 12:53:58,265: Training the random forest model
INFO: 2022-09-27 12:53:58,265: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 12:54:10,722: Saving the model
INFO: 2022-09-27 13:31:13,862: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 13:31:13,862: Splitting the data into train and test
INFO: 2022-09-27 13:31:13,905: X_train shape: (6436, 16)
INFO: 2022-09-27 13:31:13,905: X_test shape: (1609, 16)
INFO: 2022-09-27 13:31:13,905: Training the baseline model
INFO: 2022-09-27 13:31:13,905: Training the baseline model
INFO: 2022-09-27 13:31:13,910: Training the XGB model
INFO: 2022-09-27 13:31:13,910: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 13:31:14,039: Training the xgboost model
INFO: 2022-09-27 13:31:18,929: Hyper parameter tuning for random forest
INFO: 2022-09-27 13:31:18,929: Hyperparameter tuning for random forest model
INFO: 2022-09-27 13:33:06,213: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 13:33:06,214: Lowest RMSE for random forest model: 0.7138738205038734
INFO: 2022-09-27 13:33:06,217: Training the random forest model
INFO: 2022-09-27 13:33:06,217: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 13:33:22,624: Saving the model
INFO: 2022-09-27 14:00:10,338: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:00:10,338: Splitting the data into train and test
INFO: 2022-09-27 14:00:10,377: X_train shape: (6436, 16)
INFO: 2022-09-27 14:00:10,378: X_test shape: (1609, 16)
INFO: 2022-09-27 14:00:10,378: Training the baseline model
INFO: 2022-09-27 14:00:10,378: Training the baseline model
INFO: 2022-09-27 14:00:10,383: Training the XGB model
INFO: 2022-09-27 14:00:10,383: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:00:10,502: Training the xgboost model
INFO: 2022-09-27 14:00:15,792: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:00:15,793: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:01:25,631: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:01:25,631: Lowest RMSE for random forest model: 0.7143592831052938
INFO: 2022-09-27 14:01:25,632: Training the random forest model
INFO: 2022-09-27 14:01:25,632: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:01:38,833: Saving the model
INFO: 2022-09-27 14:29:06,707: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:29:06,711: Splitting the data into train and test
INFO: 2022-09-27 14:29:06,754: X_train shape: (6436, 15)
INFO: 2022-09-27 14:29:06,754: X_test shape: (1609, 15)
INFO: 2022-09-27 14:29:06,754: Training the baseline model
INFO: 2022-09-27 14:29:06,755: Training the baseline model
INFO: 2022-09-27 14:29:06,761: Training the XGB model
INFO: 2022-09-27 14:29:06,761: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:29:06,880: Training the xgboost model
INFO: 2022-09-27 14:29:11,249: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:29:11,250: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:30:32,943: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:30:32,943: Lowest RMSE for random forest model: 0.7154748707246532
INFO: 2022-09-27 14:30:32,944: Training the random forest model
INFO: 2022-09-27 14:30:32,944: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:30:43,758: Saving the model
INFO: 2022-09-27 14:32:14,922: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:32:14,922: Splitting the data into train and test
INFO: 2022-09-27 14:32:14,956: X_train shape: (6436, 15)
INFO: 2022-09-27 14:32:14,956: X_test shape: (1609, 15)
INFO: 2022-09-27 14:32:14,956: Training the baseline model
INFO: 2022-09-27 14:32:14,956: Training the baseline model
INFO: 2022-09-27 14:32:14,958: Training the XGB model
INFO: 2022-09-27 14:32:14,958: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:32:15,021: Training the xgboost model
INFO: 2022-09-27 14:32:19,419: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:32:19,420: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:33:35,121: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:33:35,121: Lowest RMSE for random forest model: 0.7175292909608055
INFO: 2022-09-27 14:33:35,122: Training the random forest model
INFO: 2022-09-27 14:33:35,122: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:33:39,869: Saving the model
INFO: 2022-09-27 14:49:58,369: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 14:49:58,369: Splitting the data into train and test
INFO: 2022-09-27 14:49:58,399: X_train shape: (6436, 14)
INFO: 2022-09-27 14:49:58,399: X_test shape: (1609, 14)
INFO: 2022-09-27 14:49:58,400: Training the baseline model
INFO: 2022-09-27 14:49:58,400: Training the baseline model
INFO: 2022-09-27 14:49:58,409: Training the XGB model
INFO: 2022-09-27 14:49:58,409: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 14:49:58,520: Training the xgboost model
INFO: 2022-09-27 14:50:02,984: Hyper parameter tuning for random forest
INFO: 2022-09-27 14:50:02,984: Hyperparameter tuning for random forest model
INFO: 2022-09-27 14:51:33,566: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:51:33,567: Lowest RMSE for random forest model: 0.7173453473023734
INFO: 2022-09-27 14:51:33,570: Training the random forest model
INFO: 2022-09-27 14:51:33,570: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 14:51:46,001: Saving the model
INFO: 2022-09-27 15:02:25,659: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:02:25,659: Splitting the data into train and test
INFO: 2022-09-27 15:02:25,692: X_train shape: (6436, 21)
INFO: 2022-09-27 15:02:25,693: X_test shape: (1609, 21)
INFO: 2022-09-27 15:02:25,693: Training the baseline model
INFO: 2022-09-27 15:02:25,693: Training the baseline model
INFO: 2022-09-27 15:02:25,697: Training the XGB model
INFO: 2022-09-27 15:02:25,697: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:02:25,770: Training the xgboost model
INFO: 2022-09-27 15:02:29,494: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:02:29,495: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:03:18,853: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:03:18,853: Lowest RMSE for random forest model: 0.7128611966836764
INFO: 2022-09-27 15:03:18,853: Training the random forest model
INFO: 2022-09-27 15:03:18,853: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:03:22,201: Saving the model
INFO: 2022-09-27 15:13:07,702: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:13:07,703: Splitting the data into train and test
INFO: 2022-09-27 15:13:07,734: X_train shape: (6436, 20)
INFO: 2022-09-27 15:13:07,734: X_test shape: (1609, 20)
INFO: 2022-09-27 15:13:07,734: Training the baseline model
INFO: 2022-09-27 15:13:07,734: Training the baseline model
INFO: 2022-09-27 15:13:07,737: Training the XGB model
INFO: 2022-09-27 15:13:07,737: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:13:07,817: Training the xgboost model
INFO: 2022-09-27 15:13:11,086: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:13:11,086: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:14:28,249: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:14:28,251: Lowest RMSE for random forest model: 0.7118586182604604
INFO: 2022-09-27 15:14:28,259: Training the random forest model
INFO: 2022-09-27 15:14:28,259: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:14:38,948: Saving the model
INFO: 2022-09-27 15:49:39,432: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 15:49:39,432: Splitting the data into train and test
INFO: 2022-09-27 15:49:39,464: X_train shape: (6436, 20)
INFO: 2022-09-27 15:49:39,464: X_test shape: (1609, 20)
INFO: 2022-09-27 15:49:39,464: Training the baseline model
INFO: 2022-09-27 15:49:39,464: Training the baseline model
INFO: 2022-09-27 15:49:39,468: Training the XGB model
INFO: 2022-09-27 15:49:39,468: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 15:49:39,553: Training the xgboost model
INFO: 2022-09-27 15:49:43,048: Hyper parameter tuning for random forest
INFO: 2022-09-27 15:49:43,048: Hyperparameter tuning for random forest model
INFO: 2022-09-27 15:50:44,909: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:50:44,909: Lowest RMSE for random forest model: 0.7145944838910999
INFO: 2022-09-27 15:50:44,910: Training the random forest model
INFO: 2022-09-27 15:50:44,910: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 15:50:49,017: Saving the model
INFO: 2022-09-27 20:32:23,447: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-27 20:32:23,450: Splitting the data into train and test
INFO: 2022-09-27 20:32:23,483: X_train shape: (6436, 22)
INFO: 2022-09-27 20:32:23,483: X_test shape: (1609, 22)
INFO: 2022-09-27 20:32:23,483: Training the baseline model
INFO: 2022-09-27 20:32:23,483: Training the baseline model
INFO: 2022-09-27 20:32:23,491: Training the XGB model
INFO: 2022-09-27 20:32:23,491: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-27 20:32:23,578: Training the xgboost model
INFO: 2022-09-27 20:32:27,982: Hyper parameter tuning for random forest
INFO: 2022-09-27 20:32:27,982: Hyperparameter tuning for random forest model
INFO: 2022-09-27 20:33:44,277: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 20:33:44,277: Lowest RMSE for random forest model: 0.7100508829052756
INFO: 2022-09-27 20:33:44,278: Training the random forest model
INFO: 2022-09-27 20:33:44,278: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-27 20:33:54,366: Saving the model
INFO: 2022-09-28 07:54:44,538: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 07:54:44,539: Splitting the data into train and test
INFO: 2022-09-28 07:54:44,581: X_train shape: (6436, 22)
INFO: 2022-09-28 07:54:44,582: X_test shape: (1609, 22)
INFO: 2022-09-28 07:54:44,582: Training the XGB model
INFO: 2022-09-28 07:54:44,582: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 07:54:44,725: Training the xgboost model
INFO: 2022-09-28 07:54:49,938: Hyper parameter tuning for random forest
INFO: 2022-09-28 07:54:49,938: Hyperparameter tuning for random forest model
ERROR: 2022-09-28 07:54:52,044: exception calling callback for <Future at 0x28a84b169a0 state=finished raised TerminatedWorkerError>
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.

INFO: 2022-09-28 08:38:12,541: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 08:38:12,542: Splitting the data into train and test
INFO: 2022-09-28 08:38:12,594: X_train shape: (6436, 23)
INFO: 2022-09-28 08:38:12,594: X_test shape: (1609, 23)
INFO: 2022-09-28 08:38:12,594: Training the baseline model
INFO: 2022-09-28 08:38:12,594: Training the baseline model
INFO: 2022-09-28 08:38:12,600: Training the XGB model
INFO: 2022-09-28 08:38:12,601: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 08:38:12,768: Training the xgboost model
INFO: 2022-09-28 08:38:17,502: Hyper parameter tuning for random forest
INFO: 2022-09-28 08:38:17,502: Hyperparameter tuning for random forest model
INFO: 2022-09-28 08:39:31,766: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 08:39:31,766: Lowest RMSE for random forest model: 0.7124156090349977
INFO: 2022-09-28 08:39:31,769: Training the random forest model
INFO: 2022-09-28 08:39:31,769: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 08:39:42,172: Saving the model
INFO: 2022-09-28 09:01:25,205: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 09:01:25,206: Splitting the data into train and test
INFO: 2022-09-28 09:01:25,250: X_train shape: (6436, 21)
INFO: 2022-09-28 09:01:25,250: X_test shape: (1609, 21)
INFO: 2022-09-28 09:01:25,250: Training the baseline model
INFO: 2022-09-28 09:01:25,251: Training the baseline model
INFO: 2022-09-28 09:01:25,258: Training the XGB model
INFO: 2022-09-28 09:01:25,258: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 09:01:25,431: Training the xgboost model
INFO: 2022-09-28 09:01:31,592: Hyper parameter tuning for random forest
INFO: 2022-09-28 09:01:31,593: Hyperparameter tuning for random forest model
INFO: 2022-09-28 09:02:51,992: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 09:02:51,994: Lowest RMSE for random forest model: 0.7142888692400011
INFO: 2022-09-28 09:02:52,001: Training the random forest model
INFO: 2022-09-28 09:02:52,001: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 09:03:02,977: Saving the model
INFO: 2022-09-28 09:20:07,923: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 09:21:54,206: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 19:57:24,835: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 19:57:24,835: Splitting the data into train and test
INFO: 2022-09-28 19:57:24,863: X_train shape: (6436, 17)
INFO: 2022-09-28 19:57:24,863: X_test shape: (1609, 17)
INFO: 2022-09-28 19:57:24,864: Training the baseline model
INFO: 2022-09-28 19:57:24,864: Training the baseline model
INFO: 2022-09-28 19:57:24,869: Training the XGB model
INFO: 2022-09-28 19:57:24,869: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 19:57:25,010: Training the xgboost model
INFO: 2022-09-28 19:57:29,539: Hyper parameter tuning for random forest
INFO: 2022-09-28 19:57:29,540: Hyperparameter tuning for random forest model
INFO: 2022-09-28 19:58:29,243: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 19:58:29,243: Lowest RMSE for random forest model: 0.7290249345760205
INFO: 2022-09-28 19:58:29,244: Training the random forest model
INFO: 2022-09-28 19:58:29,244: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 19:58:39,417: Saving the model
INFO: 2022-09-28 20:14:05,703: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:14:05,706: Splitting the data into train and test
INFO: 2022-09-28 20:14:05,731: X_train shape: (6436, 17)
INFO: 2022-09-28 20:14:05,731: X_test shape: (1609, 17)
INFO: 2022-09-28 20:14:05,731: Training the baseline model
INFO: 2022-09-28 20:14:05,731: Training the baseline model
INFO: 2022-09-28 20:14:05,735: Training the XGB model
INFO: 2022-09-28 20:14:05,735: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:14:05,846: Training the xgboost model
INFO: 2022-09-28 20:14:10,042: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:14:10,042: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:14:55,229: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:14:55,229: Lowest RMSE for random forest model: 0.734937581056365
INFO: 2022-09-28 20:14:55,230: Training the random forest model
INFO: 2022-09-28 20:14:55,230: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:14:58,903: Saving the model
INFO: 2022-09-28 20:29:34,059: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:29:34,080: Splitting the data into train and test
INFO: 2022-09-28 20:29:34,115: X_train shape: (6436, 17)
INFO: 2022-09-28 20:29:34,115: X_test shape: (1609, 17)
INFO: 2022-09-28 20:29:34,115: Training the baseline model
INFO: 2022-09-28 20:29:34,115: Training the baseline model
INFO: 2022-09-28 20:29:34,120: Training the XGB model
INFO: 2022-09-28 20:29:34,120: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:29:34,281: Training the xgboost model
INFO: 2022-09-28 20:29:40,998: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:29:40,999: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:31:13,149: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:31:13,152: Lowest RMSE for random forest model: 0.7342929450312508
INFO: 2022-09-28 20:31:13,167: Training the random forest model
INFO: 2022-09-28 20:31:13,167: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:31:22,582: Saving the model
INFO: 2022-09-28 20:33:57,717: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:33:57,717: Splitting the data into train and test
INFO: 2022-09-28 20:33:57,750: X_train shape: (6436, 17)
INFO: 2022-09-28 20:33:57,750: X_test shape: (1609, 17)
INFO: 2022-09-28 20:33:57,750: Training the baseline model
INFO: 2022-09-28 20:33:57,750: Training the baseline model
INFO: 2022-09-28 20:33:57,753: Training the XGB model
INFO: 2022-09-28 20:33:57,753: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:33:57,864: Training the xgboost model
INFO: 2022-09-28 20:34:02,828: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:34:02,828: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:35:15,819: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-28 20:35:15,820: Lowest RMSE for random forest model: 0.7387118314251307
INFO: 2022-09-28 20:35:15,821: Training the random forest model
INFO: 2022-09-28 20:35:15,821: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-28 20:35:27,343: Saving the model
INFO: 2022-09-28 20:38:21,116: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 20:38:21,116: Splitting the data into train and test
INFO: 2022-09-28 20:38:21,143: X_train shape: (6436, 17)
INFO: 2022-09-28 20:38:21,143: X_test shape: (1609, 17)
INFO: 2022-09-28 20:38:21,144: Training the baseline model
INFO: 2022-09-28 20:38:21,144: Training the baseline model
INFO: 2022-09-28 20:38:21,147: Training the XGB model
INFO: 2022-09-28 20:38:21,147: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 20:38:21,229: Training the xgboost model
INFO: 2022-09-28 20:38:25,565: Hyper parameter tuning for random forest
INFO: 2022-09-28 20:38:25,565: Hyperparameter tuning for random forest model
INFO: 2022-09-28 20:39:27,907: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:39:27,907: Lowest RMSE for random forest model: 0.7347712969533984
INFO: 2022-09-28 20:39:27,907: Training the random forest model
INFO: 2022-09-28 20:39:27,907: Best parameters for random forest model: {'n_estimators': 500, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 20:39:36,720: Saving the model
INFO: 2022-09-28 21:06:15,774: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-28 21:06:15,774: Splitting the data into train and test
INFO: 2022-09-28 21:06:15,801: X_train shape: (6436, 17)
INFO: 2022-09-28 21:06:15,801: X_test shape: (1609, 17)
INFO: 2022-09-28 21:06:15,801: Training the baseline model
INFO: 2022-09-28 21:06:15,801: Training the baseline model
INFO: 2022-09-28 21:06:15,807: Training the XGB model
INFO: 2022-09-28 21:06:15,807: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-28 21:06:15,924: Training the xgboost model
INFO: 2022-09-28 21:06:20,525: Hyper parameter tuning for random forest
INFO: 2022-09-28 21:06:20,525: Hyperparameter tuning for random forest model
INFO: 2022-09-28 21:07:27,262: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 21:07:27,262: Lowest RMSE for random forest model: 0.7338636554002407
INFO: 2022-09-28 21:07:27,263: Training the random forest model
INFO: 2022-09-28 21:07:27,263: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-28 21:07:31,476: Saving the model
INFO: 2022-09-29 08:56:24,704: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-29 08:56:24,705: Splitting the data into train and test
INFO: 2022-09-29 08:56:24,741: X_train shape: (6436, 19)
INFO: 2022-09-29 08:56:24,741: X_test shape: (1609, 19)
INFO: 2022-09-29 08:56:24,741: Training the baseline model
INFO: 2022-09-29 08:56:24,741: Training the baseline model
INFO: 2022-09-29 08:56:24,748: Training the XGB model
INFO: 2022-09-29 08:56:24,748: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-29 08:56:24,917: Training the xgboost model
INFO: 2022-09-29 08:56:30,128: Hyper parameter tuning for random forest
INFO: 2022-09-29 08:56:30,129: Hyperparameter tuning for random forest model
INFO: 2022-09-29 08:57:28,336: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-29 08:57:28,336: Lowest RMSE for random forest model: 0.7342271901376728
INFO: 2022-09-29 08:57:28,337: Training the random forest model
INFO: 2022-09-29 08:57:28,337: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-29 08:57:32,051: Saving the model
INFO: 2022-09-30 00:11:52,914: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:12:43,362: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:12:43,363: Hyperparameter tuning for xgboost model
INFO: 2022-09-30 00:13:43,034: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:13:43,034: Hyperparameter tuning for xgboost model
INFO: 2022-09-30 00:14:17,371: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:14:17,371: Hyperparameter tuning for xgboost model
ERROR: 2022-09-30 00:14:21,854: exception calling callback for <Future at 0x22c1c327a90 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 407, in _process_worker
    call_item = call_queue.get(block=True, timeout=timeout)
  File "c:\python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\__init__.py", line 6, in <module>
    from .core import (
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 231, in <module>
    _LIB = _load_lib()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 184, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (xgboost.dll) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['[WinError 1455] The paging file is too small for this operation to complete']

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-09-30 00:15:37,366: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:15:37,366: Hyperparameter tuning for xgboost model
ERROR: 2022-09-30 00:15:41,579: exception calling callback for <Future at 0x2df3cfd5a60 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 407, in _process_worker
    call_item = call_queue.get(block=True, timeout=timeout)
  File "c:\python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\__init__.py", line 6, in <module>
    from .core import (
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 231, in <module>
    _LIB = _load_lib()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\xgboost\core.py", line 184, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (xgboost.dll) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['[WinError 1455] The paging file is too small for this operation to complete']

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\final_solution\code_challenge\myenv\lib\site-packages\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-09-30 00:17:36,756: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:17:36,756: Splitting the data into train and test
INFO: 2022-09-30 00:17:36,790: X_train shape: (6436, 19)
INFO: 2022-09-30 00:17:36,790: X_test shape: (1609, 19)
INFO: 2022-09-30 00:17:36,790: Training the baseline model
INFO: 2022-09-30 00:17:36,790: Training the baseline model
INFO: 2022-09-30 00:17:36,796: Training the XGB model
INFO: 2022-09-30 00:17:36,796: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:17:36,938: Training the xgboost model
INFO: 2022-09-30 00:17:42,508: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:17:42,509: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:18:21,095: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:18:21,095: Lowest RMSE for random forest model: 0.747074571237682
INFO: 2022-09-30 00:18:21,096: Training the random forest model
INFO: 2022-09-30 00:18:21,096: Best parameters for random forest model: {'n_estimators': 100, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:18:24,373: Saving the model
INFO: 2022-09-30 00:19:00,275: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:19:00,275: Splitting the data into train and test
INFO: 2022-09-30 00:19:00,306: X_train shape: (6436, 19)
INFO: 2022-09-30 00:19:00,307: X_test shape: (1609, 19)
INFO: 2022-09-30 00:19:00,307: Training the baseline model
INFO: 2022-09-30 00:19:00,307: Training the baseline model
INFO: 2022-09-30 00:19:00,310: Training the XGB model
INFO: 2022-09-30 00:19:00,310: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:19:00,409: Training the xgboost model
INFO: 2022-09-30 00:19:05,470: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:19:05,471: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:19:51,576: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-30 00:19:51,576: Lowest RMSE for random forest model: 0.7353151789339766
INFO: 2022-09-30 00:19:51,577: Training the random forest model
INFO: 2022-09-30 00:19:51,577: Best parameters for random forest model: {'n_estimators': 200, 'max_features': 2, 'bootstrap': True}
INFO: 2022-09-30 00:19:55,613: Saving the model
INFO: 2022-09-30 00:53:27,642: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 00:53:27,643: Splitting the data into train and test
INFO: 2022-09-30 00:53:27,689: X_train shape: (6436, 19)
INFO: 2022-09-30 00:53:27,689: X_test shape: (1609, 19)
INFO: 2022-09-30 00:53:27,689: Training the baseline model
INFO: 2022-09-30 00:53:27,690: Training the baseline model
INFO: 2022-09-30 00:53:27,696: Training the XGB model
INFO: 2022-09-30 00:53:27,696: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 00:53:27,834: Training the xgboost model
INFO: 2022-09-30 00:53:33,797: Hyper parameter tuning for random forest
INFO: 2022-09-30 00:53:33,797: Hyperparameter tuning for random forest model
INFO: 2022-09-30 00:54:40,963: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:54:40,964: Lowest RMSE for random forest model: 0.7468120524712232
INFO: 2022-09-30 00:54:40,964: Training the random forest model
INFO: 2022-09-30 00:54:40,964: Best parameters for random forest model: {'n_estimators': 600, 'max_features': 3, 'bootstrap': True}
INFO: 2022-09-30 00:54:52,912: Saving the model
INFO: 2022-09-30 01:20:14,811: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 01:20:14,811: Splitting the data into train and test
INFO: 2022-09-30 01:20:14,841: X_train shape: (6436, 19)
INFO: 2022-09-30 01:20:14,841: X_test shape: (1609, 19)
INFO: 2022-09-30 01:20:14,841: Training the baseline model
INFO: 2022-09-30 01:20:14,842: Training the baseline model
INFO: 2022-09-30 01:20:14,844: Training the XGB model
INFO: 2022-09-30 01:20:14,844: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 01:20:14,949: Training the xgboost model
INFO: 2022-09-30 01:20:20,336: Hyper parameter tuning for random forest
INFO: 2022-09-30 01:20:20,336: Hyperparameter tuning for random forest model
INFO: 2022-09-30 01:21:02,326: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-09-30 01:21:02,326: Lowest RMSE for random forest model: 0.7246445986076976
INFO: 2022-09-30 01:21:02,327: Training the random forest model
INFO: 2022-09-30 01:21:02,327: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-09-30 01:21:03,218: Saving the model
INFO: 2022-09-30 01:22:13,331: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 01:22:13,332: Splitting the data into train and test
INFO: 2022-09-30 01:22:13,370: X_train shape: (6436, 19)
INFO: 2022-09-30 01:22:13,370: X_test shape: (1609, 19)
INFO: 2022-09-30 01:22:13,370: Training the baseline model
INFO: 2022-09-30 01:22:13,370: Training the baseline model
INFO: 2022-09-30 01:22:13,373: Training the XGB model
INFO: 2022-09-30 01:22:13,374: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 01:22:13,487: Training the xgboost model
INFO: 2022-09-30 01:22:19,001: Hyper parameter tuning for random forest
INFO: 2022-09-30 01:22:19,001: Hyperparameter tuning for random forest model
INFO: 2022-09-30 01:23:16,771: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 01:23:16,771: Lowest RMSE for random forest model: 0.7244819633075281
INFO: 2022-09-30 01:23:16,771: Training the random forest model
INFO: 2022-09-30 01:23:16,772: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 01:23:21,715: Saving the model
INFO: 2022-09-30 09:14:38,222: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 09:14:38,222: Splitting the data into train and test
INFO: 2022-09-30 09:14:38,266: X_train shape: (6436, 19)
INFO: 2022-09-30 09:14:38,267: X_test shape: (1609, 19)
INFO: 2022-09-30 09:14:38,267: Training the baseline model
INFO: 2022-09-30 09:14:38,267: Training the baseline model
INFO: 2022-09-30 09:14:38,279: Training the XGB model
INFO: 2022-09-30 09:14:38,279: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 09:14:38,704: Training the xgboost model
INFO: 2022-09-30 09:14:46,208: Hyper parameter tuning for random forest
INFO: 2022-09-30 09:14:46,208: Hyperparameter tuning for random forest model
INFO: 2022-09-30 09:16:02,029: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 09:16:02,029: Lowest RMSE for random forest model: 0.7277483264706763
INFO: 2022-09-30 09:16:02,030: Training the random forest model
INFO: 2022-09-30 09:16:02,030: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-09-30 09:16:09,887: Saving the model
INFO: 2022-09-30 11:06:12,899: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 11:06:12,902: Splitting the data into train and test
INFO: 2022-09-30 11:06:12,932: X_train shape: (6436, 17)
INFO: 2022-09-30 11:06:12,932: X_test shape: (1609, 17)
INFO: 2022-09-30 11:06:12,932: Training the baseline model
INFO: 2022-09-30 11:06:12,932: Training the baseline model
INFO: 2022-09-30 11:06:12,938: Training the XGB model
INFO: 2022-09-30 11:06:12,938: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 11:06:13,074: Training the xgboost model
INFO: 2022-09-30 11:06:19,222: Hyper parameter tuning for random forest
INFO: 2022-09-30 11:06:19,222: Hyperparameter tuning for random forest model
INFO: 2022-09-30 11:07:15,515: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 30, 'bootstrap': True}
INFO: 2022-09-30 11:07:15,515: Lowest RMSE for random forest model: 0.730701941404418
INFO: 2022-09-30 11:07:15,516: Training the random forest model
INFO: 2022-09-30 11:07:15,516: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 30, 'bootstrap': True}
INFO: 2022-09-30 11:07:21,471: Saving the model
INFO: 2022-09-30 11:36:05,972: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 11:36:05,973: Splitting the data into train and test
INFO: 2022-09-30 11:36:06,007: X_train shape: (6436, 17)
INFO: 2022-09-30 11:36:06,007: X_test shape: (1609, 17)
INFO: 2022-09-30 11:36:06,007: Training the baseline model
INFO: 2022-09-30 11:36:06,007: Training the baseline model
INFO: 2022-09-30 11:36:06,012: Training the XGB model
INFO: 2022-09-30 11:36:06,013: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 11:36:06,097: Training the xgboost model
INFO: 2022-09-30 11:36:11,325: Hyper parameter tuning for random forest
INFO: 2022-09-30 11:36:11,325: Hyperparameter tuning for random forest model
INFO: 2022-09-30 11:37:27,964: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-09-30 11:37:27,965: Lowest RMSE for random forest model: 0.7171871726894145
INFO: 2022-09-30 11:37:27,965: Training the random forest model
INFO: 2022-09-30 11:37:27,965: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-09-30 11:37:39,369: Saving the model
INFO: 2022-09-30 13:46:05,766: Reading the final data from the data/processed folder and training the model
INFO: 2022-09-30 13:46:05,767: Splitting the data into train and test
INFO: 2022-09-30 13:46:05,795: X_train shape: (6436, 17)
INFO: 2022-09-30 13:46:05,795: X_test shape: (1609, 17)
INFO: 2022-09-30 13:46:05,796: Training the baseline model
INFO: 2022-09-30 13:46:05,796: Training the baseline model
INFO: 2022-09-30 13:46:05,800: Training the XGB model
INFO: 2022-09-30 13:46:05,800: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-09-30 13:46:05,945: Training the xgboost model
INFO: 2022-09-30 13:46:10,847: Hyper parameter tuning for random forest
INFO: 2022-09-30 13:46:10,847: Hyperparameter tuning for random forest model
INFO: 2022-09-30 13:47:36,114: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-09-30 13:47:36,114: Lowest RMSE for random forest model: 0.7238512860742443
INFO: 2022-09-30 13:47:36,116: Training the random forest model
INFO: 2022-09-30 13:47:36,116: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-09-30 13:47:46,290: Saving the model
INFO: 2022-10-01 13:06:16,931: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-01 13:06:16,932: Splitting the data into train and test
INFO: 2022-10-01 13:06:16,958: X_train shape: (6436, 17)
INFO: 2022-10-01 13:06:16,958: X_test shape: (1609, 17)
INFO: 2022-10-01 13:06:16,958: Training the baseline model
INFO: 2022-10-01 13:06:16,958: Training the baseline model
INFO: 2022-10-01 13:06:16,964: Training the XGB model
INFO: 2022-10-01 13:06:16,964: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-01 13:06:17,073: Training the xgboost model
INFO: 2022-10-01 13:06:21,222: Hyper parameter tuning for random forest
INFO: 2022-10-01 13:06:21,222: Hyperparameter tuning for random forest model
INFO: 2022-10-01 13:07:17,063: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-01 13:07:17,063: Lowest RMSE for random forest model: 0.7242812981300921
INFO: 2022-10-01 13:07:17,063: Training the random forest model
INFO: 2022-10-01 13:07:17,063: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-01 13:07:20,359: Saving the model
INFO: 2022-10-03 09:20:11,469: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-03 09:20:11,470: Splitting the data into train and test
INFO: 2022-10-03 09:20:11,494: X_train shape: (6436, 17)
INFO: 2022-10-03 09:20:11,494: X_test shape: (1609, 17)
INFO: 2022-10-03 09:20:11,494: Training the baseline model
INFO: 2022-10-03 09:20:11,494: Training the baseline model
INFO: 2022-10-03 09:20:11,499: Training the XGB model
INFO: 2022-10-03 09:20:11,499: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-03 09:20:11,588: Training the xgboost model
INFO: 2022-10-03 09:20:15,871: Hyper parameter tuning for random forest
INFO: 2022-10-03 09:20:15,872: Hyperparameter tuning for random forest model
INFO: 2022-10-03 09:21:08,050: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-03 09:21:08,050: Lowest RMSE for random forest model: 0.7133584435289362
INFO: 2022-10-03 09:21:08,051: Training the random forest model
INFO: 2022-10-03 09:21:08,051: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-03 09:21:09,121: Saving the model
INFO: 2022-10-03 09:52:31,529: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-03 09:52:31,529: Splitting the data into train and test
INFO: 2022-10-03 09:52:31,558: X_train shape: (6436, 17)
INFO: 2022-10-03 09:52:31,558: X_test shape: (1609, 17)
INFO: 2022-10-03 09:52:31,558: Training the baseline model
INFO: 2022-10-03 09:52:31,558: Training the baseline model
INFO: 2022-10-03 09:52:31,564: Training the XGB model
INFO: 2022-10-03 09:52:31,564: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-03 09:52:31,763: Training the xgboost model
INFO: 2022-10-03 09:52:35,973: Hyper parameter tuning for random forest
INFO: 2022-10-03 09:52:35,973: Hyperparameter tuning for random forest model
INFO: 2022-10-03 09:53:38,410: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-03 09:53:38,411: Lowest RMSE for random forest model: 0.7182718024711234
INFO: 2022-10-03 09:53:38,411: Training the random forest model
INFO: 2022-10-03 09:53:38,412: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-03 09:53:45,653: Saving the model
INFO: 2022-10-03 10:29:24,496: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-03 10:29:24,497: Splitting the data into train and test
INFO: 2022-10-03 10:29:24,522: X_train shape: (6436, 17)
INFO: 2022-10-03 10:29:24,522: X_test shape: (1609, 17)
INFO: 2022-10-03 10:29:24,522: Training the baseline model
INFO: 2022-10-03 10:29:24,522: Training the baseline model
INFO: 2022-10-03 10:29:24,526: Training the XGB model
INFO: 2022-10-03 10:29:24,526: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-03 10:29:24,726: Training the xgboost model
INFO: 2022-10-03 10:29:29,248: Hyper parameter tuning for random forest
INFO: 2022-10-03 10:29:29,248: Hyperparameter tuning for random forest model
INFO: 2022-10-03 10:30:30,664: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-03 10:30:30,665: Lowest RMSE for random forest model: 0.7430927884535418
INFO: 2022-10-03 10:30:30,665: Training the random forest model
INFO: 2022-10-03 10:30:30,665: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-03 10:30:44,582: Saving the model
INFO: 2022-10-03 15:39:02,296: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-03 15:39:02,302: Splitting the data into train and test
INFO: 2022-10-03 15:39:02,419: X_train shape: (6436, 17)
INFO: 2022-10-03 15:39:02,419: X_test shape: (1609, 17)
INFO: 2022-10-03 15:39:02,419: Training the baseline model
INFO: 2022-10-03 15:39:02,420: Training the baseline model
INFO: 2022-10-03 15:39:02,458: Training the XGB model
INFO: 2022-10-03 15:39:02,458: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-03 15:39:02,878: Training the xgboost model
INFO: 2022-10-03 15:39:07,962: Hyper parameter tuning for random forest
INFO: 2022-10-03 15:39:07,963: Hyperparameter tuning for random forest model
INFO: 2022-10-03 15:40:03,426: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-03 15:40:03,426: Lowest RMSE for random forest model: 0.7179103687828975
INFO: 2022-10-03 15:40:05,004: Training the random forest model
INFO: 2022-10-03 15:40:05,005: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-03 15:40:10,184: Saving the model
INFO: 2022-10-04 10:43:21,260: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 10:43:21,261: Splitting the data into train and test
INFO: 2022-10-04 10:43:21,286: X_train shape: (6436, 17)
INFO: 2022-10-04 10:43:21,286: X_test shape: (1609, 17)
INFO: 2022-10-04 10:43:21,287: Training the baseline model
INFO: 2022-10-04 10:43:21,287: Training the baseline model
INFO: 2022-10-04 10:43:21,291: Training the XGB model
INFO: 2022-10-04 10:43:21,291: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 10:43:21,490: Training the xgboost model
INFO: 2022-10-04 10:43:25,322: Hyper parameter tuning for random forest
INFO: 2022-10-04 10:43:25,323: Hyperparameter tuning for random forest model
INFO: 2022-10-04 10:44:11,247: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 10:44:11,247: Lowest RMSE for random forest model: 0.7183295759558644
INFO: 2022-10-04 10:44:11,248: Training the random forest model
INFO: 2022-10-04 10:44:11,248: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 10:44:12,093: Saving the model
INFO: 2022-10-04 13:58:18,023: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 13:58:18,024: Splitting the data into train and test
INFO: 2022-10-04 13:58:18,049: X_train shape: (6436, 17)
INFO: 2022-10-04 13:58:18,049: X_test shape: (1609, 17)
INFO: 2022-10-04 13:58:18,049: Training the baseline model
INFO: 2022-10-04 13:58:18,049: Training the baseline model
INFO: 2022-10-04 13:58:18,054: Training the XGB model
INFO: 2022-10-04 13:58:18,054: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 13:58:18,260: Training the xgboost model
INFO: 2022-10-04 13:58:22,427: Hyper parameter tuning for random forest
INFO: 2022-10-04 13:58:22,427: Hyperparameter tuning for random forest model
INFO: 2022-10-04 13:59:10,781: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-04 13:59:10,783: Lowest RMSE for random forest model: 0.7114602794500214
INFO: 2022-10-04 13:59:10,794: Training the random forest model
INFO: 2022-10-04 13:59:10,795: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-04 13:59:12,206: Saving the model
INFO: 2022-10-04 14:00:25,814: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 14:00:25,815: Splitting the data into train and test
INFO: 2022-10-04 14:00:25,839: X_train shape: (6436, 17)
INFO: 2022-10-04 14:00:25,839: X_test shape: (1609, 17)
INFO: 2022-10-04 14:00:25,839: Training the baseline model
INFO: 2022-10-04 14:00:25,840: Training the baseline model
INFO: 2022-10-04 14:00:25,842: Training the XGB model
INFO: 2022-10-04 14:00:25,842: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 14:00:25,934: Training the xgboost model
INFO: 2022-10-04 14:00:29,792: Hyper parameter tuning for random forest
INFO: 2022-10-04 14:00:29,792: Hyperparameter tuning for random forest model
INFO: 2022-10-04 14:01:22,343: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 14:01:22,343: Lowest RMSE for random forest model: 0.7223680932156048
INFO: 2022-10-04 14:01:22,344: Training the random forest model
INFO: 2022-10-04 14:01:22,344: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 14:01:23,920: Saving the model
INFO: 2022-10-04 15:01:51,126: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 15:01:51,127: Splitting the data into train and test
INFO: 2022-10-04 15:01:51,150: X_train shape: (6436, 17)
INFO: 2022-10-04 15:01:51,151: X_test shape: (1609, 17)
INFO: 2022-10-04 15:01:51,151: Training the baseline model
INFO: 2022-10-04 15:01:51,151: Training the baseline model
INFO: 2022-10-04 15:01:51,154: Training the XGB model
INFO: 2022-10-04 15:01:51,154: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 15:01:51,366: Training the xgboost model
INFO: 2022-10-04 15:01:55,716: Hyper parameter tuning for random forest
INFO: 2022-10-04 15:01:55,717: Hyperparameter tuning for random forest model
INFO: 2022-10-04 15:02:43,712: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 15:02:43,713: Lowest RMSE for random forest model: 0.7186681721103031
INFO: 2022-10-04 15:02:43,713: Training the random forest model
INFO: 2022-10-04 15:02:43,713: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-04 15:02:50,930: Saving the model
INFO: 2022-10-04 15:07:32,210: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 15:07:32,215: Splitting the data into train and test
INFO: 2022-10-04 15:07:32,256: X_train shape: (6436, 17)
INFO: 2022-10-04 15:07:32,256: X_test shape: (1609, 17)
INFO: 2022-10-04 15:07:32,256: Training the baseline model
INFO: 2022-10-04 15:07:32,256: Training the baseline model
INFO: 2022-10-04 15:07:32,261: Training the XGB model
INFO: 2022-10-04 15:07:32,261: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 15:07:32,445: Training the xgboost model
INFO: 2022-10-04 15:07:36,874: Hyper parameter tuning for random forest
INFO: 2022-10-04 15:07:36,874: Hyperparameter tuning for random forest model
INFO: 2022-10-04 15:09:38,849: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-04 15:09:38,852: Lowest RMSE for random forest model: 0.7241160990859191
INFO: 2022-10-04 15:09:38,861: Training the random forest model
INFO: 2022-10-04 15:09:38,862: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-04 15:09:53,868: Saving the model
INFO: 2022-10-04 15:42:40,478: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-04 15:42:40,479: Splitting the data into train and test
INFO: 2022-10-04 15:42:40,528: X_train shape: (6436, 17)
INFO: 2022-10-04 15:42:40,528: X_test shape: (1609, 17)
INFO: 2022-10-04 15:42:40,528: Training the baseline model
INFO: 2022-10-04 15:42:40,529: Training the baseline model
INFO: 2022-10-04 15:42:40,533: Training the XGB model
INFO: 2022-10-04 15:42:40,533: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-04 15:42:40,755: Training the xgboost model
INFO: 2022-10-04 15:42:44,522: Hyper parameter tuning for random forest
INFO: 2022-10-04 15:42:44,522: Hyperparameter tuning for random forest model
INFO: 2022-10-04 15:44:10,708: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-04 15:44:10,708: Lowest RMSE for random forest model: 0.7090964437543196
INFO: 2022-10-04 15:44:10,709: Training the random forest model
INFO: 2022-10-04 15:44:10,709: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-04 15:44:11,521: Saving the model
INFO: 2022-10-05 11:48:38,142: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 11:48:38,143: Splitting the data into train and test
INFO: 2022-10-05 11:48:38,186: X_train shape: (6436, 17)
INFO: 2022-10-05 11:48:38,187: X_test shape: (1609, 17)
INFO: 2022-10-05 11:48:38,187: Training the baseline model
INFO: 2022-10-05 11:48:38,187: Training the baseline model
INFO: 2022-10-05 11:48:38,192: Training the XGB model
INFO: 2022-10-05 11:48:38,192: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 11:48:38,436: Training the xgboost model
INFO: 2022-10-05 11:48:41,881: Hyper parameter tuning for random forest
INFO: 2022-10-05 11:48:41,881: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:10:24,764: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:10:24,765: Splitting the data into train and test
INFO: 2022-10-05 12:10:24,808: X_train shape: (6436, 17)
INFO: 2022-10-05 12:10:24,808: X_test shape: (1609, 17)
INFO: 2022-10-05 12:10:24,808: Training the baseline model
INFO: 2022-10-05 12:10:24,809: Training the baseline model
INFO: 2022-10-05 12:10:24,813: Training the XGB model
INFO: 2022-10-05 12:10:24,814: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:10:25,015: Training the xgboost model
INFO: 2022-10-05 12:10:29,572: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:10:29,572: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:11:50,505: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-05 12:11:50,507: Lowest RMSE for random forest model: 0.705331565070557
INFO: 2022-10-05 12:11:50,514: Training the random forest model
INFO: 2022-10-05 12:11:50,514: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-05 12:11:55,133: Saving the model
INFO: 2022-10-05 12:22:16,934: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:22:16,934: Splitting the data into train and test
INFO: 2022-10-05 12:22:16,983: X_train shape: (6436, 17)
INFO: 2022-10-05 12:22:16,983: X_test shape: (1609, 17)
INFO: 2022-10-05 12:22:16,983: Training the baseline model
INFO: 2022-10-05 12:22:16,983: Training the baseline model
INFO: 2022-10-05 12:22:16,986: Training the XGB model
INFO: 2022-10-05 12:22:16,986: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:22:17,079: Training the xgboost model
INFO: 2022-10-05 12:22:21,508: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:22:21,508: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:23:22,628: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-05 12:23:22,628: Lowest RMSE for random forest model: 0.7138392126688579
INFO: 2022-10-05 12:23:22,629: Training the random forest model
INFO: 2022-10-05 12:23:22,629: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-05 12:23:28,125: Saving the model
INFO: 2022-10-05 12:25:19,083: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:25:19,083: Splitting the data into train and test
INFO: 2022-10-05 12:25:19,124: X_train shape: (6436, 17)
INFO: 2022-10-05 12:25:19,124: X_test shape: (1609, 17)
INFO: 2022-10-05 12:25:19,124: Training the baseline model
INFO: 2022-10-05 12:25:19,124: Training the baseline model
INFO: 2022-10-05 12:25:19,126: Training the XGB model
INFO: 2022-10-05 12:25:19,126: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:25:19,186: Training the xgboost model
INFO: 2022-10-05 12:25:22,997: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:25:22,997: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:26:33,632: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:26:33,632: Lowest RMSE for random forest model: 0.7161324232436406
INFO: 2022-10-05 12:26:33,632: Training the random forest model
INFO: 2022-10-05 12:26:33,633: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:26:39,845: Saving the model
INFO: 2022-10-05 12:29:17,997: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:29:17,997: Splitting the data into train and test
INFO: 2022-10-05 12:29:18,038: X_train shape: (6436, 17)
INFO: 2022-10-05 12:29:18,038: X_test shape: (1609, 17)
INFO: 2022-10-05 12:29:18,038: Training the baseline model
INFO: 2022-10-05 12:29:18,038: Training the baseline model
INFO: 2022-10-05 12:29:18,041: Training the XGB model
INFO: 2022-10-05 12:29:18,041: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:29:18,106: Training the xgboost model
INFO: 2022-10-05 12:29:22,115: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:29:22,115: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:30:46,436: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-05 12:30:46,436: Lowest RMSE for random forest model: 0.7116845903795677
INFO: 2022-10-05 12:30:46,437: Training the random forest model
INFO: 2022-10-05 12:30:46,437: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-05 12:30:52,824: Saving the model
INFO: 2022-10-05 12:34:26,135: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:34:26,135: Splitting the data into train and test
INFO: 2022-10-05 12:34:26,175: X_train shape: (6436, 17)
INFO: 2022-10-05 12:34:26,175: X_test shape: (1609, 17)
INFO: 2022-10-05 12:34:26,175: Training the baseline model
INFO: 2022-10-05 12:34:26,175: Training the baseline model
INFO: 2022-10-05 12:34:26,177: Training the XGB model
INFO: 2022-10-05 12:34:26,177: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:34:26,240: Training the xgboost model
INFO: 2022-10-05 12:34:30,211: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:34:30,211: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:35:29,390: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:35:29,390: Lowest RMSE for random forest model: 0.7199658323589202
INFO: 2022-10-05 12:35:29,391: Training the random forest model
INFO: 2022-10-05 12:35:29,391: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:35:31,389: Saving the model
INFO: 2022-10-05 12:37:20,448: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:37:20,448: Splitting the data into train and test
INFO: 2022-10-05 12:37:20,489: X_train shape: (6436, 17)
INFO: 2022-10-05 12:37:20,489: X_test shape: (1609, 17)
INFO: 2022-10-05 12:37:20,489: Training the baseline model
INFO: 2022-10-05 12:37:20,489: Training the baseline model
INFO: 2022-10-05 12:37:20,491: Training the XGB model
INFO: 2022-10-05 12:37:20,491: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:37:20,555: Training the xgboost model
INFO: 2022-10-05 12:37:24,739: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:37:24,739: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:38:32,547: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:38:32,548: Lowest RMSE for random forest model: 0.7157218095547239
INFO: 2022-10-05 12:38:32,548: Training the random forest model
INFO: 2022-10-05 12:38:32,548: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:38:40,016: Saving the model
INFO: 2022-10-05 12:41:20,443: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:41:20,443: Splitting the data into train and test
INFO: 2022-10-05 12:41:20,484: X_train shape: (6436, 17)
INFO: 2022-10-05 12:41:20,485: X_test shape: (1609, 17)
INFO: 2022-10-05 12:41:20,485: Training the baseline model
INFO: 2022-10-05 12:41:20,485: Training the baseline model
INFO: 2022-10-05 12:41:20,487: Training the XGB model
INFO: 2022-10-05 12:41:20,487: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:41:20,547: Training the xgboost model
INFO: 2022-10-05 12:41:24,626: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:41:24,627: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:43:07,349: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 12:43:07,349: Lowest RMSE for random forest model: 0.713519922496555
INFO: 2022-10-05 12:43:07,349: Training the random forest model
INFO: 2022-10-05 12:43:07,350: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 12:43:17,874: Saving the model
INFO: 2022-10-05 12:51:48,373: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:51:48,374: Splitting the data into train and test
INFO: 2022-10-05 12:51:48,413: X_train shape: (6436, 17)
INFO: 2022-10-05 12:51:48,413: X_test shape: (1609, 17)
INFO: 2022-10-05 12:51:48,413: Training the baseline model
INFO: 2022-10-05 12:51:48,413: Training the baseline model
INFO: 2022-10-05 12:51:48,416: Training the XGB model
INFO: 2022-10-05 12:51:48,416: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:51:48,485: Training the xgboost model
INFO: 2022-10-05 12:51:52,571: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:51:52,571: Hyperparameter tuning for random forest model
INFO: 2022-10-05 12:53:07,085: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:53:07,086: Lowest RMSE for random forest model: 0.7102974330397631
INFO: 2022-10-05 12:53:07,086: Training the random forest model
INFO: 2022-10-05 12:53:07,086: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 12:53:13,276: Saving the model
INFO: 2022-10-05 12:58:54,115: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 12:58:54,115: Splitting the data into train and test
INFO: 2022-10-05 12:58:54,155: X_train shape: (6436, 17)
INFO: 2022-10-05 12:58:54,155: X_test shape: (1609, 17)
INFO: 2022-10-05 12:58:54,155: Training the baseline model
INFO: 2022-10-05 12:58:54,155: Training the baseline model
INFO: 2022-10-05 12:58:54,157: Training the XGB model
INFO: 2022-10-05 12:58:54,158: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 12:58:54,223: Training the xgboost model
INFO: 2022-10-05 12:58:58,135: Hyper parameter tuning for random forest
INFO: 2022-10-05 12:58:58,135: Hyperparameter tuning for random forest model
INFO: 2022-10-05 13:00:19,626: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 13:00:19,627: Lowest RMSE for random forest model: 0.7135042495700883
INFO: 2022-10-05 13:00:19,627: Training the random forest model
INFO: 2022-10-05 13:00:19,627: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 13:00:30,867: Saving the model
INFO: 2022-10-05 13:05:12,851: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 13:05:12,851: Splitting the data into train and test
INFO: 2022-10-05 13:05:12,892: X_train shape: (6436, 17)
INFO: 2022-10-05 13:05:12,892: X_test shape: (1609, 17)
INFO: 2022-10-05 13:05:12,892: Training the baseline model
INFO: 2022-10-05 13:05:12,892: Training the baseline model
INFO: 2022-10-05 13:05:12,895: Training the XGB model
INFO: 2022-10-05 13:05:12,895: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 13:05:12,975: Training the xgboost model
INFO: 2022-10-05 13:05:17,027: Hyper parameter tuning for random forest
INFO: 2022-10-05 13:05:17,027: Hyperparameter tuning for random forest model
INFO: 2022-10-05 13:06:47,145: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-05 13:06:47,148: Lowest RMSE for random forest model: 0.7167197569501156
INFO: 2022-10-05 13:06:47,155: Training the random forest model
INFO: 2022-10-05 13:06:47,155: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-05 13:06:48,356: Saving the model
INFO: 2022-10-05 13:09:46,123: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 13:09:46,123: Splitting the data into train and test
INFO: 2022-10-05 13:09:46,163: X_train shape: (6436, 17)
INFO: 2022-10-05 13:09:46,163: X_test shape: (1609, 17)
INFO: 2022-10-05 13:09:46,163: Training the baseline model
INFO: 2022-10-05 13:09:46,163: Training the baseline model
INFO: 2022-10-05 13:09:46,166: Training the XGB model
INFO: 2022-10-05 13:09:46,166: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 13:09:46,284: Training the xgboost model
INFO: 2022-10-05 13:09:50,626: Hyper parameter tuning for random forest
INFO: 2022-10-05 13:09:50,626: Hyperparameter tuning for random forest model
INFO: 2022-10-05 13:11:09,081: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-05 13:11:09,081: Lowest RMSE for random forest model: 0.7124729917029466
INFO: 2022-10-05 13:11:09,081: Training the random forest model
INFO: 2022-10-05 13:11:09,081: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-05 13:11:15,936: Saving the model
INFO: 2022-10-05 14:05:47,921: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 14:05:47,921: Splitting the data into train and test
INFO: 2022-10-05 14:05:47,963: X_train shape: (6436, 17)
INFO: 2022-10-05 14:05:47,963: X_test shape: (1609, 17)
INFO: 2022-10-05 14:05:47,963: Training the baseline model
INFO: 2022-10-05 14:05:47,963: Training the baseline model
INFO: 2022-10-05 14:05:47,969: Training the XGB model
INFO: 2022-10-05 14:05:47,969: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 14:05:48,197: Training the xgboost model
INFO: 2022-10-05 14:05:52,412: Hyper parameter tuning for random forest
INFO: 2022-10-05 14:05:52,412: Hyperparameter tuning for random forest model
INFO: 2022-10-05 14:07:35,068: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-05 14:07:35,069: Lowest RMSE for random forest model: 0.7117937804370575
INFO: 2022-10-05 14:07:35,073: Training the random forest model
INFO: 2022-10-05 14:07:35,073: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-05 14:07:41,678: Saving the model
INFO: 2022-10-05 14:10:39,559: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 14:10:39,559: Splitting the data into train and test
INFO: 2022-10-05 14:10:39,604: X_train shape: (6436, 17)
INFO: 2022-10-05 14:10:39,604: X_test shape: (1609, 17)
INFO: 2022-10-05 14:10:39,604: Training the baseline model
INFO: 2022-10-05 14:10:39,604: Training the baseline model
INFO: 2022-10-05 14:10:39,607: Training the XGB model
INFO: 2022-10-05 14:10:39,607: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 14:10:39,729: Training the xgboost model
INFO: 2022-10-05 14:10:43,790: Hyper parameter tuning for random forest
INFO: 2022-10-05 14:10:43,790: Hyperparameter tuning for random forest model
INFO: 2022-10-05 14:11:56,540: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 14:11:56,541: Lowest RMSE for random forest model: 0.7106167347299351
INFO: 2022-10-05 14:11:56,541: Training the random forest model
INFO: 2022-10-05 14:11:56,541: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-05 14:11:58,786: Saving the model
INFO: 2022-10-05 14:28:10,836: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-05 14:28:10,836: Splitting the data into train and test
INFO: 2022-10-05 14:28:10,875: X_train shape: (6436, 17)
INFO: 2022-10-05 14:28:10,875: X_test shape: (1609, 17)
INFO: 2022-10-05 14:28:10,875: Training the baseline model
INFO: 2022-10-05 14:28:10,875: Training the baseline model
INFO: 2022-10-05 14:28:10,879: Training the XGB model
INFO: 2022-10-05 14:28:10,879: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-05 14:28:11,118: Training the xgboost model
INFO: 2022-10-05 14:28:16,058: Hyper parameter tuning for random forest
INFO: 2022-10-05 14:28:16,058: Hyperparameter tuning for random forest model
INFO: 2022-10-05 14:29:21,998: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 14:29:21,998: Lowest RMSE for random forest model: 0.7120334331606898
INFO: 2022-10-05 14:29:21,999: Training the random forest model
INFO: 2022-10-05 14:29:21,999: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-05 14:29:24,165: Saving the model
INFO: 2022-10-06 09:25:20,366: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 09:25:20,366: Splitting the data into train and test
INFO: 2022-10-06 09:25:20,410: X_train shape: (6436, 17)
INFO: 2022-10-06 09:25:20,410: X_test shape: (1609, 17)
INFO: 2022-10-06 09:25:20,410: Training the baseline model
INFO: 2022-10-06 09:25:20,410: Training the baseline model
INFO: 2022-10-06 09:25:20,417: Training the XGB model
INFO: 2022-10-06 09:25:20,417: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 09:25:20,606: Training the xgboost model
INFO: 2022-10-06 09:25:25,348: Hyper parameter tuning for random forest
INFO: 2022-10-06 09:25:25,348: Hyperparameter tuning for random forest model
INFO: 2022-10-06 09:27:01,122: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-06 09:27:01,122: Lowest RMSE for random forest model: 0.7134000664449027
INFO: 2022-10-06 09:27:01,123: Training the random forest model
INFO: 2022-10-06 09:27:01,123: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-06 09:27:13,756: Saving the model
INFO: 2022-10-06 11:30:05,975: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 11:30:05,976: Splitting the data into train and test
INFO: 2022-10-06 11:30:06,014: X_train shape: (6436, 17)
INFO: 2022-10-06 11:30:06,014: X_test shape: (1609, 17)
INFO: 2022-10-06 11:30:06,014: Training the baseline model
INFO: 2022-10-06 11:30:06,014: Training the baseline model
INFO: 2022-10-06 11:30:06,020: Training the XGB model
INFO: 2022-10-06 11:30:06,020: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 11:30:06,140: Training the xgboost model
INFO: 2022-10-06 11:30:10,363: Hyper parameter tuning for random forest
INFO: 2022-10-06 11:30:10,363: Hyperparameter tuning for random forest model
INFO: 2022-10-06 11:31:57,918: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 11:31:57,918: Lowest RMSE for random forest model: 0.7164004908331263
INFO: 2022-10-06 11:31:57,923: Training the random forest model
INFO: 2022-10-06 11:31:57,923: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 11:32:03,730: Saving the model
INFO: 2022-10-06 12:09:38,515: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 12:09:38,515: Splitting the data into train and test
INFO: 2022-10-06 12:09:38,554: X_train shape: (6436, 17)
INFO: 2022-10-06 12:09:38,554: X_test shape: (1609, 17)
INFO: 2022-10-06 12:09:38,554: Training the baseline model
INFO: 2022-10-06 12:09:38,555: Training the baseline model
INFO: 2022-10-06 12:09:38,560: Training the XGB model
INFO: 2022-10-06 12:09:38,560: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 12:09:38,739: Training the xgboost model
INFO: 2022-10-06 12:09:43,020: Hyper parameter tuning for random forest
INFO: 2022-10-06 12:09:43,021: Hyperparameter tuning for random forest model
INFO: 2022-10-06 12:10:59,268: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 12:10:59,270: Lowest RMSE for random forest model: 0.7197491763649644
INFO: 2022-10-06 12:10:59,281: Training the random forest model
INFO: 2022-10-06 12:10:59,281: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 12:11:00,346: Saving the model
INFO: 2022-10-06 17:13:48,686: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:13:48,686: Splitting the data into train and test
INFO: 2022-10-06 17:13:48,690: X_train shape: (5148, 17)
INFO: 2022-10-06 17:13:48,690: X_test shape: (1609, 17)
INFO: 2022-10-06 17:13:48,690: Training the baseline model
INFO: 2022-10-06 17:13:48,690: Training the baseline model
INFO: 2022-10-06 17:13:48,696: Training the XGB model
INFO: 2022-10-06 17:14:18,919: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:14:18,919: Splitting the data into train and test
INFO: 2022-10-06 17:14:18,922: X_train shape: (5148, 17)
INFO: 2022-10-06 17:14:18,923: X_test shape: (1609, 17)
INFO: 2022-10-06 17:14:18,923: Training the baseline model
INFO: 2022-10-06 17:14:18,923: Training the baseline model
INFO: 2022-10-06 17:14:18,924: Training the XGB model
INFO: 2022-10-06 17:16:27,969: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:16:27,969: Splitting the data into train and test
INFO: 2022-10-06 17:16:27,973: X_train shape: (5148, 17)
INFO: 2022-10-06 17:16:27,973: X_test shape: (1609, 17)
INFO: 2022-10-06 17:16:27,973: Training the baseline model
INFO: 2022-10-06 17:16:27,973: Training the baseline model
INFO: 2022-10-06 17:16:27,975: Training the XGB model
INFO: 2022-10-06 17:16:41,219: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 17:16:41,219: Best score for XGBoost: 0.33442453000216127
INFO: 2022-10-06 17:16:41,238: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 17:17:14,997: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:17:14,997: Splitting the data into train and test
INFO: 2022-10-06 17:17:15,002: X_train shape: (5148, 17)
INFO: 2022-10-06 17:17:15,003: X_test shape: (1609, 17)
INFO: 2022-10-06 17:17:15,003: Training the baseline model
INFO: 2022-10-06 17:17:15,003: Training the baseline model
INFO: 2022-10-06 17:17:15,004: Training the XGB model
INFO: 2022-10-06 17:17:28,463: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 17:17:28,464: Best score for XGBoost: 0.35753693433150513
INFO: 2022-10-06 17:17:28,469: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 17:17:28,469: Training the XGBoost model
INFO: 2022-10-06 17:18:10,640: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:18:10,640: Splitting the data into train and test
INFO: 2022-10-06 17:18:10,645: X_train shape: (5148, 17)
INFO: 2022-10-06 17:18:10,645: X_test shape: (1609, 17)
INFO: 2022-10-06 17:18:10,645: Training the baseline model
INFO: 2022-10-06 17:18:10,645: Training the baseline model
INFO: 2022-10-06 17:18:10,647: Training the XGB model
INFO: 2022-10-06 17:18:24,041: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 17:18:24,041: Best score for XGBoost: 0.36361144819430635
INFO: 2022-10-06 17:18:24,046: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 17:18:24,049: Training the XGBoost model
INFO: 2022-10-06 17:18:52,334: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:18:52,334: Splitting the data into train and test
INFO: 2022-10-06 17:18:52,338: X_train shape: (5148, 17)
INFO: 2022-10-06 17:18:52,338: X_test shape: (1609, 17)
INFO: 2022-10-06 17:18:52,338: Training the baseline model
INFO: 2022-10-06 17:18:52,338: Training the baseline model
INFO: 2022-10-06 17:18:52,340: Training the XGB model
INFO: 2022-10-06 17:19:05,922: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 17:19:05,922: Best score for XGBoost: 0.3532163967070064
INFO: 2022-10-06 17:19:05,927: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 17:19:05,931: Training the XGBoost model
INFO: 2022-10-06 17:21:58,762: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:21:58,763: Splitting the data into train and test
INFO: 2022-10-06 17:21:58,766: X_train shape: (5148, 17)
INFO: 2022-10-06 17:21:58,767: X_test shape: (1609, 17)
INFO: 2022-10-06 17:21:58,767: Training the baseline model
INFO: 2022-10-06 17:21:58,767: Training the baseline model
INFO: 2022-10-06 17:21:58,769: Training the XGB model
INFO: 2022-10-06 17:22:12,384: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 17:22:12,385: Best score for XGBoost: 0.33241171006589576
INFO: 2022-10-06 17:22:12,390: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 17:22:12,390: Training the XGBoost model
INFO: 2022-10-06 17:29:54,541: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:29:54,542: Splitting the data into train and test
INFO: 2022-10-06 17:30:09,608: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:30:09,608: Splitting the data into train and test
INFO: 2022-10-06 17:30:09,612: Training the baseline model
INFO: 2022-10-06 17:30:09,612: Training the baseline model
INFO: 2022-10-06 17:30:32,967: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 17:30:32,967: Splitting the data into train and test
INFO: 2022-10-06 17:30:32,971: Training the baseline model
INFO: 2022-10-06 17:30:32,971: Training the baseline model
INFO: 2022-10-06 18:10:52,606: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:10:52,609: Splitting the data into train and test
INFO: 2022-10-06 18:10:52,613: Training the baseline model
INFO: 2022-10-06 18:10:52,613: Training the baseline model
INFO: 2022-10-06 18:11:16,276: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:11:16,276: Splitting the data into train and test
INFO: 2022-10-06 18:11:16,280: Training the baseline model
INFO: 2022-10-06 18:11:16,280: Training the baseline model
INFO: 2022-10-06 18:11:45,997: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:11:45,997: Splitting the data into train and test
INFO: 2022-10-06 18:11:46,002: Training the baseline model
INFO: 2022-10-06 18:11:46,002: Training the baseline model
INFO: 2022-10-06 18:13:40,795: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:13:40,796: Splitting the data into train and test
INFO: 2022-10-06 18:13:40,800: Training the baseline model
INFO: 2022-10-06 18:13:40,801: Training the baseline model
INFO: 2022-10-06 18:13:40,803: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:15:08,325: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:15:08,325: Splitting the data into train and test
INFO: 2022-10-06 18:15:08,328: Training the baseline model
INFO: 2022-10-06 18:15:08,329: Training the baseline model
INFO: 2022-10-06 18:15:08,331: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:15:32,965: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:15:32,965: Splitting the data into train and test
INFO: 2022-10-06 18:15:32,969: Training the baseline model
INFO: 2022-10-06 18:15:32,969: Training the baseline model
INFO: 2022-10-06 18:16:36,165: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:16:36,165: Splitting the data into train and test
INFO: 2022-10-06 18:16:36,170: Training the baseline model
INFO: 2022-10-06 18:16:36,170: Training the baseline model
INFO: 2022-10-06 18:17:09,869: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:17:09,869: Splitting the data into train and test
INFO: 2022-10-06 18:17:09,878: Training the baseline model
INFO: 2022-10-06 18:17:09,878: Training the baseline model
INFO: 2022-10-06 18:17:25,282: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:17:25,282: Splitting the data into train and test
INFO: 2022-10-06 18:17:25,286: Training the baseline model
INFO: 2022-10-06 18:17:25,286: Training the baseline model
INFO: 2022-10-06 18:17:25,288: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:20:50,357: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:20:50,357: Splitting the data into train and test
INFO: 2022-10-06 18:21:13,715: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:21:13,715: Splitting the data into train and test
INFO: 2022-10-06 18:21:13,719: Training the baseline model
INFO: 2022-10-06 18:21:13,719: Training the baseline model
INFO: 2022-10-06 18:21:13,721: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:23:31,248: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:23:31,249: Splitting the data into train and test
INFO: 2022-10-06 18:23:31,252: Training the baseline model
INFO: 2022-10-06 18:23:31,252: Training the baseline model
INFO: 2022-10-06 18:23:31,255: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:25:29,976: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:25:29,976: Splitting the data into train and test
INFO: 2022-10-06 18:25:29,980: Training the baseline model
INFO: 2022-10-06 18:25:29,980: Training the baseline model
INFO: 2022-10-06 18:25:29,982: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:26:56,857: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:26:56,857: Splitting the data into train and test
INFO: 2022-10-06 18:26:56,861: Training the baseline model
INFO: 2022-10-06 18:26:56,861: Training the baseline model
INFO: 2022-10-06 18:26:56,863: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:27:10,268: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:27:10,268: Best score for XGBoost: 0.35860543680193996
INFO: 2022-10-06 18:27:10,274: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:28:29,241: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:28:29,241: Splitting the data into train and test
INFO: 2022-10-06 18:28:29,245: Training the baseline model
INFO: 2022-10-06 18:28:29,245: Training the baseline model
INFO: 2022-10-06 18:28:29,247: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:28:44,764: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:28:44,764: Best score for XGBoost: 0.34634317688151883
INFO: 2022-10-06 18:28:44,770: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:30:12,764: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:30:12,764: Splitting the data into train and test
INFO: 2022-10-06 18:30:12,769: Training the baseline model
INFO: 2022-10-06 18:30:12,769: Training the baseline model
INFO: 2022-10-06 18:30:12,770: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:30:27,030: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:30:27,030: Best score for XGBoost: 0.33859448362816086
INFO: 2022-10-06 18:30:27,035: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:30:27,036: Training the XGBoost model
INFO: 2022-10-06 18:31:28,068: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:31:28,068: Splitting the data into train and test
INFO: 2022-10-06 18:31:28,072: Training the baseline model
INFO: 2022-10-06 18:31:28,072: Training the baseline model
INFO: 2022-10-06 18:31:28,073: Converting to DMatrix and splitting the data in 80:10:10 ratio
INFO: 2022-10-06 18:31:41,681: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:31:41,681: Best score for XGBoost: 0.3239213128218062
INFO: 2022-10-06 18:31:41,686: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:31:41,686: Training the XGBoost model
INFO: 2022-10-06 18:36:18,878: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:36:18,879: Splitting the data into train and test
INFO: 2022-10-06 18:36:18,883: Training the baseline model
INFO: 2022-10-06 18:36:18,883: Training the baseline model
INFO: 2022-10-06 18:36:18,885: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 18:36:32,311: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:36:32,311: Best score for XGBoost: 0.3214641071565586
INFO: 2022-10-06 18:36:32,317: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:36:32,317: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 18:36:32,317: Training the XGBoost model
INFO: 2022-10-06 18:36:32,803: Hyper parameter tuning for random forest
INFO: 2022-10-06 18:36:32,803: Hyperparameter tuning for random forest model
INFO: 2022-10-06 18:38:29,084: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-06 18:38:29,085: Lowest RMSE for random forest model: 0.7140948311421595
INFO: 2022-10-06 18:38:29,091: Training the random forest model
INFO: 2022-10-06 18:39:52,311: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:39:52,311: Splitting the data into train and test
INFO: 2022-10-06 18:39:52,314: Training the baseline model
INFO: 2022-10-06 18:39:52,314: Training the baseline model
INFO: 2022-10-06 18:39:52,320: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 18:40:08,882: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:40:08,882: Best score for XGBoost: 0.33268121670078193
INFO: 2022-10-06 18:40:08,892: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:40:08,892: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 18:40:08,892: Training the XGBoost model
INFO: 2022-10-06 18:40:09,374: Hyper parameter tuning for random forest
INFO: 2022-10-06 18:40:09,374: Hyperparameter tuning for random forest model
INFO: 2022-10-06 18:41:36,136: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-06 18:41:36,137: Lowest RMSE for random forest model: 0.7141309359553216
INFO: 2022-10-06 18:41:36,137: Training the random forest model
INFO: 2022-10-06 18:42:06,229: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:42:06,229: Splitting the data into train and test
INFO: 2022-10-06 18:42:06,233: Training the baseline model
INFO: 2022-10-06 18:42:06,233: Training the baseline model
INFO: 2022-10-06 18:42:06,235: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 18:42:20,136: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:42:20,136: Best score for XGBoost: 0.3504739988262132
INFO: 2022-10-06 18:42:20,142: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:42:20,142: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 18:42:20,142: Training the XGBoost model
INFO: 2022-10-06 18:42:20,716: Hyper parameter tuning for random forest
INFO: 2022-10-06 18:42:20,716: Hyperparameter tuning for random forest model
INFO: 2022-10-06 18:43:59,011: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-06 18:43:59,011: Lowest RMSE for random forest model: 0.7121060061761659
INFO: 2022-10-06 18:43:59,012: Training the random forest model
INFO: 2022-10-06 18:43:59,012: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-06 18:53:06,217: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 18:53:06,218: Splitting the data into train and test
INFO: 2022-10-06 18:53:06,227: Training the baseline model
INFO: 2022-10-06 18:53:06,227: Training the baseline model
INFO: 2022-10-06 18:53:06,234: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 18:53:19,696: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 18:53:19,696: Best score for XGBoost: 0.3353812457561557
INFO: 2022-10-06 18:53:19,703: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 18:53:19,703: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 18:53:19,703: Training the XGBoost model
INFO: 2022-10-06 18:53:20,266: Hyper parameter tuning for random forest
INFO: 2022-10-06 18:53:20,266: Hyperparameter tuning for random forest model
INFO: 2022-10-06 18:55:00,776: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-10-06 18:55:00,777: Lowest RMSE for random forest model: 0.7112324120209266
INFO: 2022-10-06 18:55:00,784: Training the random forest model
INFO: 2022-10-06 18:55:00,784: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-10-06 19:03:11,144: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:03:11,144: Splitting the data into train and test
INFO: 2022-10-06 19:03:11,152: Training the baseline model
INFO: 2022-10-06 19:03:11,152: Training the baseline model
INFO: 2022-10-06 19:03:11,158: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:03:24,816: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:03:24,816: Best score for XGBoost: 0.33955834875179897
INFO: 2022-10-06 19:03:24,823: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:03:24,823: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:03:24,823: Training the XGBoost model
INFO: 2022-10-06 19:03:25,281: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:03:25,281: Hyperparameter tuning for random forest model
INFO: 2022-10-06 19:05:06,317: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 19:05:06,317: Lowest RMSE for random forest model: 0.7141674848052921
INFO: 2022-10-06 19:05:06,317: Training the random forest model
INFO: 2022-10-06 19:05:06,318: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 19:08:49,254: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:08:49,254: Splitting the data into train and test
INFO: 2022-10-06 19:09:29,131: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:09:29,131: Splitting the data into train and test
INFO: 2022-10-06 19:09:29,144: Training the baseline model
INFO: 2022-10-06 19:09:29,144: Training the baseline model
INFO: 2022-10-06 19:09:29,146: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:09:45,856: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:09:45,856: Best score for XGBoost: 0.33131025572102357
INFO: 2022-10-06 19:09:45,862: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:09:45,862: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:09:45,862: Training the XGBoost model
INFO: 2022-10-06 19:09:46,388: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:09:46,388: Hyperparameter tuning for random forest model
INFO: 2022-10-06 19:11:05,246: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 19:11:05,246: Lowest RMSE for random forest model: 0.7162312527019664
INFO: 2022-10-06 19:11:05,247: Training the random forest model
INFO: 2022-10-06 19:11:05,247: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 19:21:35,430: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:21:35,430: Splitting the data into train and test
INFO: 2022-10-06 19:27:56,200: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:27:56,200: Splitting the data into train and test
INFO: 2022-10-06 19:28:26,268: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:28:26,268: Splitting the data into train and test
INFO: 2022-10-06 19:29:29,983: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:29:29,983: Splitting the data into train and test
INFO: 2022-10-06 19:30:05,764: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:30:05,765: Splitting the data into train and test
INFO: 2022-10-06 19:30:05,784: Training the baseline model
INFO: 2022-10-06 19:30:05,785: Training the baseline model
INFO: 2022-10-06 19:30:05,790: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:30:20,930: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:30:20,930: Best score for XGBoost: 0.33100921556088114
INFO: 2022-10-06 19:30:20,936: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:30:20,936: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:30:20,936: Training the XGBoost model
INFO: 2022-10-06 19:30:21,577: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:30:21,577: Hyperparameter tuning for random forest model
INFO: 2022-10-06 19:32:05,575: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 19:32:05,576: Lowest RMSE for random forest model: 0.7146129779097024
INFO: 2022-10-06 19:32:05,583: Training the random forest model
INFO: 2022-10-06 19:32:05,583: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 19:33:54,114: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:33:54,114: Splitting the data into train and test
INFO: 2022-10-06 19:33:54,133: Training the baseline model
INFO: 2022-10-06 19:33:54,133: Training the baseline model
INFO: 2022-10-06 19:33:54,139: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:34:08,518: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:34:08,518: Best score for XGBoost: 0.33430529997740677
INFO: 2022-10-06 19:34:08,524: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:34:08,524: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:34:08,524: Training the XGBoost model
INFO: 2022-10-06 19:34:09,055: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:34:09,055: Hyperparameter tuning for random forest model
INFO: 2022-10-06 19:36:56,003: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-10-06 19:36:56,006: Lowest RMSE for random forest model: 0.7137634323870127
INFO: 2022-10-06 19:36:56,015: Training the random forest model
INFO: 2022-10-06 19:36:56,015: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 100, 'bootstrap': True}
INFO: 2022-10-06 19:52:40,602: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:52:40,602: Splitting the data into train and test
INFO: 2022-10-06 19:53:19,527: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:53:19,527: Splitting the data into train and test
INFO: 2022-10-06 19:53:49,240: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:53:49,240: Splitting the data into train and test
INFO: 2022-10-06 19:53:49,259: Training the baseline model
INFO: 2022-10-06 19:53:49,259: Training the baseline model
INFO: 2022-10-06 19:53:49,266: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:54:03,063: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:54:03,063: Best score for XGBoost: 0.3324137379342996
INFO: 2022-10-06 19:54:03,069: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:54:03,070: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:54:03,070: Training the XGBoost model
INFO: 2022-10-06 19:54:03,534: XGB model Model saved
INFO: 2022-10-06 19:54:03,534: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:54:03,534: Hyperparameter tuning for random forest model
INFO: 2022-10-06 19:55:35,238: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-06 19:55:35,238: Lowest RMSE for random forest model: 0.7109996381337887
INFO: 2022-10-06 19:55:35,239: Training the random forest model
INFO: 2022-10-06 19:55:35,239: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-06 19:55:44,888: Random forest model saved
INFO: 2022-10-06 19:57:18,126: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:57:18,126: Splitting the data into train and test
INFO: 2022-10-06 19:57:18,145: Training the baseline model
INFO: 2022-10-06 19:57:18,145: Training the baseline model
INFO: 2022-10-06 19:57:18,147: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:57:32,097: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:57:32,098: Best score for XGBoost: 0.3264030630251383
INFO: 2022-10-06 19:57:32,103: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:57:32,103: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:57:32,103: Training the XGBoost model
INFO: 2022-10-06 19:58:43,791: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 19:58:43,792: Splitting the data into train and test
INFO: 2022-10-06 19:58:43,809: Training the baseline model
INFO: 2022-10-06 19:58:43,809: Training the baseline model
INFO: 2022-10-06 19:58:43,810: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 19:58:58,343: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 19:58:58,343: Best score for XGBoost: 0.3417477274803641
INFO: 2022-10-06 19:58:58,349: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 19:58:58,349: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 19:58:58,350: Training the XGBoost model
INFO: 2022-10-06 19:58:58,796: XGB model Model saved
INFO: 2022-10-06 19:58:58,797: Hyper parameter tuning for random forest
INFO: 2022-10-06 19:58:58,797: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:01:24,109: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-06 20:01:24,109: Lowest RMSE for random forest model: 0.7123113444385911
INFO: 2022-10-06 20:01:24,110: Training the random forest model
INFO: 2022-10-06 20:01:24,110: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-06 20:01:28,931: Random forest model saved
INFO: 2022-10-06 20:01:47,330: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:01:47,330: Splitting the data into train and test
INFO: 2022-10-06 20:01:47,353: Training the baseline model
INFO: 2022-10-06 20:01:47,353: Training the baseline model
INFO: 2022-10-06 20:01:47,354: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:02:03,018: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:02:03,018: Best score for XGBoost: 0.3402957694652195
INFO: 2022-10-06 20:02:03,141: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:02:03,141: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:02:03,142: Training the XGBoost model
INFO: 2022-10-06 20:03:07,439: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:03:07,439: Splitting the data into train and test
INFO: 2022-10-06 20:03:07,459: Training the baseline model
INFO: 2022-10-06 20:03:07,459: Training the baseline model
INFO: 2022-10-06 20:03:07,461: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:03:21,655: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:03:21,655: Best score for XGBoost: 0.34027502438317014
INFO: 2022-10-06 20:03:21,661: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:03:21,661: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:03:21,661: Training the XGBoost model
INFO: 2022-10-06 20:03:57,755: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:03:57,755: Splitting the data into train and test
INFO: 2022-10-06 20:03:57,775: Training the baseline model
INFO: 2022-10-06 20:03:57,775: Training the baseline model
INFO: 2022-10-06 20:03:57,777: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:04:12,528: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:04:12,528: Best score for XGBoost: 0.346394516830382
INFO: 2022-10-06 20:04:12,534: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:04:12,535: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:04:12,535: Training the XGBoost model
INFO: 2022-10-06 20:04:13,081: XGB model Model saved
INFO: 2022-10-06 20:04:13,081: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:04:13,081: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:04:44,078: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:04:44,078: Lowest RMSE for random forest model: 0.7080529308006331
INFO: 2022-10-06 20:04:44,079: Training the random forest model
INFO: 2022-10-06 20:04:44,079: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:04:44,768: Random forest model saved
INFO: 2022-10-06 20:05:13,923: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:05:13,923: Splitting the data into train and test
INFO: 2022-10-06 20:05:13,943: Training the baseline model
INFO: 2022-10-06 20:05:13,943: Training the baseline model
INFO: 2022-10-06 20:05:13,945: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:05:28,115: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:05:28,116: Best score for XGBoost: 0.3389199521019862
INFO: 2022-10-06 20:05:28,122: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:05:28,122: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:05:28,122: Training the XGBoost model
INFO: 2022-10-06 20:05:28,604: XGB model Model saved
INFO: 2022-10-06 20:05:28,604: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:05:28,604: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:06:09,330: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-06 20:06:09,331: Lowest RMSE for random forest model: 0.7154098827525639
INFO: 2022-10-06 20:06:09,331: Training the random forest model
INFO: 2022-10-06 20:06:09,331: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-06 20:06:11,120: Random forest model saved
INFO: 2022-10-06 20:18:19,466: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:18:19,467: Splitting the data into train and test
INFO: 2022-10-06 20:18:19,485: Training the baseline model
INFO: 2022-10-06 20:18:19,485: Training the baseline model
INFO: 2022-10-06 20:18:19,487: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:18:33,675: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:18:33,675: Best score for XGBoost: 0.33955092022304856
INFO: 2022-10-06 20:18:33,681: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:18:33,681: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:18:33,682: Training the XGBoost model
INFO: 2022-10-06 20:18:34,208: XGB model Model saved
INFO: 2022-10-06 20:18:34,208: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:18:34,208: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:19:08,982: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:19:08,982: Lowest RMSE for random forest model: 0.7015322502704242
INFO: 2022-10-06 20:19:08,982: Training the random forest model
INFO: 2022-10-06 20:19:08,982: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:19:09,737: Random forest model saved
INFO: 2022-10-06 20:24:53,947: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:24:53,947: Splitting the data into train and test
INFO: 2022-10-06 20:24:53,971: Training the baseline model
INFO: 2022-10-06 20:24:53,971: Training the baseline model
INFO: 2022-10-06 20:24:53,972: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:25:08,092: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:25:08,092: Best score for XGBoost: 0.34422254233021804
INFO: 2022-10-06 20:25:08,098: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:25:08,098: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:25:08,098: Training the XGBoost model
INFO: 2022-10-06 20:25:08,569: XGB model Model saved
INFO: 2022-10-06 20:25:08,569: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:25:08,569: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:25:47,032: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:25:47,032: Lowest RMSE for random forest model: 0.6988548334632896
INFO: 2022-10-06 20:25:47,033: Training the random forest model
INFO: 2022-10-06 20:25:47,033: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:25:48,473: Random forest model saved
INFO: 2022-10-06 20:26:32,886: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:26:32,886: Splitting the data into train and test
INFO: 2022-10-06 20:26:32,909: Training the baseline model
INFO: 2022-10-06 20:26:32,910: Training the baseline model
INFO: 2022-10-06 20:26:32,911: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:26:46,884: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0, 'colsample_bytree': 0.3}
INFO: 2022-10-06 20:26:46,884: Best score for XGBoost: 0.34287762786106873
INFO: 2022-10-06 20:26:46,890: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:26:46,890: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:26:46,890: Training the XGBoost model
INFO: 2022-10-06 20:26:47,385: XGB model Model saved
INFO: 2022-10-06 20:26:47,385: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:26:47,385: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:27:27,976: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:27:27,976: Lowest RMSE for random forest model: 0.698846832474021
INFO: 2022-10-06 20:27:27,977: Training the random forest model
INFO: 2022-10-06 20:27:27,977: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-06 20:27:29,362: Random forest model saved
INFO: 2022-10-06 20:34:33,329: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-06 20:34:33,329: Splitting the data into train and test
INFO: 2022-10-06 20:34:33,352: Training the baseline model
INFO: 2022-10-06 20:34:33,352: Training the baseline model
INFO: 2022-10-06 20:34:33,354: Hyperparameter tuning of XGBoost model
INFO: 2022-10-06 20:34:43,492: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-06 20:34:43,492: Best score for XGBoost: 0.39111203809674244
INFO: 2022-10-06 20:34:43,498: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-06 20:34:43,498: Training the XGBoost model with best hyperparameters
INFO: 2022-10-06 20:34:43,498: Training the XGBoost model
INFO: 2022-10-06 20:34:44,289: XGB model Model saved
INFO: 2022-10-06 20:34:44,290: Hyper parameter tuning for random forest
INFO: 2022-10-06 20:34:44,290: Hyperparameter tuning for random forest model
INFO: 2022-10-06 20:36:13,123: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 20:36:13,123: Lowest RMSE for random forest model: 0.705273825332978
INFO: 2022-10-06 20:36:13,124: Training the random forest model
INFO: 2022-10-06 20:36:13,124: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 90, 'bootstrap': True}
INFO: 2022-10-06 20:36:14,048: Random forest model saved
INFO: 2022-10-07 00:22:15,790: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 00:22:15,791: Splitting the data into train and test
INFO: 2022-10-07 00:22:15,820: Training the baseline model
INFO: 2022-10-07 00:22:15,820: Training the baseline model
INFO: 2022-10-07 00:22:15,871: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 00:22:25,721: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 00:22:25,721: Best score for XGBoost: 0.3913888455668692
INFO: 2022-10-07 00:22:25,727: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 00:22:25,728: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 00:22:25,728: Training the XGBoost model
INFO: 2022-10-07 00:22:26,193: XGB model Model saved
INFO: 2022-10-07 00:22:26,194: Hyper parameter tuning for random forest
INFO: 2022-10-07 00:22:26,194: Hyperparameter tuning for random forest model
ERROR: 2022-10-07 00:22:45,224: exception calling callback for <Future at 0x15a3c9411c0 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\process_executor.py", line 407, in _process_worker
  File "C:\Python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\__init__.py", line 48, in <module>
    from pandas.core.api import (
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\core\api.py", line 29, in <module>
    from pandas.core.arrays import Categorical
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\core\arrays\__init__.py", line 11, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 844, in exec_module
  File "<frozen importlib._bootstrap_external>", line 939, in get_code
  File "<frozen importlib._bootstrap_external>", line 1038, in get_data
MemoryError
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-10-07 00:23:43,916: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 00:23:43,916: Splitting the data into train and test
INFO: 2022-10-07 00:23:43,944: Training the baseline model
INFO: 2022-10-07 00:23:43,945: Training the baseline model
INFO: 2022-10-07 00:23:43,946: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 00:23:54,316: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 00:23:54,316: Best score for XGBoost: 0.3897331431687676
INFO: 2022-10-07 00:23:54,322: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 00:23:54,322: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 00:23:54,322: Training the XGBoost model
INFO: 2022-10-07 00:23:54,810: XGB model Model saved
INFO: 2022-10-07 00:23:54,810: Hyper parameter tuning for random forest
INFO: 2022-10-07 00:23:54,810: Hyperparameter tuning for random forest model
ERROR: 2022-10-07 00:24:13,703: exception calling callback for <Future at 0x240be183970 state=finished raised BrokenProcessPool>
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\process_executor.py", line 407, in _process_worker
    call_item = call_queue.get(block=True, timeout=timeout)
  File "C:\Python38\lib\multiprocessing\queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\__init__.py", line 48, in <module>
    from pandas.core.api import (
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\core\api.py", line 29, in <module>
    from pandas.core.arrays import Categorical
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\core\arrays\__init__.py", line 11, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\pandas-1.4.4-py3.8-win-amd64.egg\pandas\core\arrays\interval.py", line 90, in <module>
    from pandas.core.indexes.base import ensure_index
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 844, in exec_module
  File "<frozen importlib._bootstrap_external>", line 939, in get_code
  File "<frozen importlib._bootstrap_external>", line 1038, in get_data
MemoryError
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\_base.py", line 625, in _invoke_callbacks
    callback(self)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 359, in __call__
    self.parallel.dispatch_next()
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 794, in dispatch_next
    if not self.dispatch_one_batch(self._original_iterator):
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 861, in dispatch_one_batch
    self._dispatch(tasks)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\parallel.py", line 779, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\_parallel_backends.py", line 531, in apply_async
    future = self._workers.submit(SafeFunction(func))
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\reusable_executor.py", line 177, in submit
    return super(_ReusablePoolExecutor, self).submit(
  File "D:\27_september\3october\code_challenge\venv\lib\site-packages\joblib-1.1.0-py3.8.egg\joblib\externals\loky\process_executor.py", line 1115, in submit
    raise self._flags.broken
joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.
INFO: 2022-10-07 00:29:33,610: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 00:29:36,290: Splitting the data into train and test
INFO: 2022-10-07 00:29:36,321: Training the baseline model
INFO: 2022-10-07 00:29:36,321: Training the baseline model
INFO: 2022-10-07 00:29:36,322: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 00:29:45,804: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 00:29:45,804: Best score for XGBoost: 0.39057018794532816
INFO: 2022-10-07 00:29:45,809: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 00:29:45,809: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 00:29:45,809: Training the XGBoost model
INFO: 2022-10-07 00:29:46,204: XGB model Model saved
INFO: 2022-10-07 00:29:46,204: Hyper parameter tuning for random forest
INFO: 2022-10-07 00:29:46,204: Hyperparameter tuning for random forest model
INFO: 2022-10-07 00:31:04,575: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 00:31:04,575: Lowest RMSE for random forest model: 0.6995309983165902
INFO: 2022-10-07 00:31:04,576: Training the random forest model
INFO: 2022-10-07 00:31:04,576: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 00:31:11,332: Random forest model saved
INFO: 2022-10-07 06:09:34,913: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 06:09:34,914: Splitting the data into train and test
INFO: 2022-10-07 06:09:34,986: Training the baseline model
INFO: 2022-10-07 06:09:34,986: Training the baseline model
INFO: 2022-10-07 06:09:34,993: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 06:09:44,694: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 06:09:44,694: Best score for XGBoost: 0.39537530676043264
INFO: 2022-10-07 06:09:44,701: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 06:09:44,702: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 06:09:44,702: Training the XGBoost model
INFO: 2022-10-07 06:09:45,141: XGB model Model saved
INFO: 2022-10-07 06:09:45,142: Hyper parameter tuning for random forest
INFO: 2022-10-07 06:09:45,142: Hyperparameter tuning for random forest model
INFO: 2022-10-07 06:11:51,962: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-07 06:11:51,963: Lowest RMSE for random forest model: 0.7058397599958003
INFO: 2022-10-07 06:11:51,972: Training the random forest model
INFO: 2022-10-07 06:11:51,972: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 60, 'bootstrap': True}
INFO: 2022-10-07 06:11:59,892: Random forest model saved
INFO: 2022-10-07 06:36:54,933: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 06:36:54,933: Splitting the data into train and test
INFO: 2022-10-07 06:36:54,949: Training the baseline model
INFO: 2022-10-07 06:36:54,949: Training the baseline model
INFO: 2022-10-07 06:36:54,952: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 06:37:03,557: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 06:37:03,558: Best score for XGBoost: 0.41896303458981105
INFO: 2022-10-07 06:37:03,567: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 06:37:03,567: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 06:37:03,567: Training the XGBoost model
INFO: 2022-10-07 06:37:03,928: XGB model Model saved
INFO: 2022-10-07 06:37:03,929: Hyper parameter tuning for random forest
INFO: 2022-10-07 06:37:03,929: Hyperparameter tuning for random forest model
INFO: 2022-10-07 06:38:29,755: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 06:38:29,755: Lowest RMSE for random forest model: 56.37305469029798
INFO: 2022-10-07 06:38:29,756: Training the random forest model
INFO: 2022-10-07 06:38:29,756: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 06:38:35,090: Random forest model saved
INFO: 2022-10-07 06:41:43,209: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 06:41:43,209: Splitting the data into train and test
INFO: 2022-10-07 06:41:43,223: Training the baseline model
INFO: 2022-10-07 06:41:43,224: Training the baseline model
INFO: 2022-10-07 06:41:43,225: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 06:41:52,666: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 06:41:52,667: Best score for XGBoost: 0.38807591265456776
INFO: 2022-10-07 06:41:52,673: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 06:41:52,673: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 06:41:52,673: Training the XGBoost model
INFO: 2022-10-07 06:41:53,105: XGB model Model saved
INFO: 2022-10-07 06:41:53,105: Hyper parameter tuning for random forest
INFO: 2022-10-07 06:41:53,106: Hyperparameter tuning for random forest model
INFO: 2022-10-07 06:43:49,096: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-07 06:43:49,096: Lowest RMSE for random forest model: 0.7041705349868865
INFO: 2022-10-07 06:43:49,097: Training the random forest model
INFO: 2022-10-07 06:43:49,097: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-07 06:43:51,393: Random forest model saved
INFO: 2022-10-07 07:32:03,730: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:32:03,731: Splitting the data into train and test
INFO: 2022-10-07 07:32:03,806: Training the baseline model
INFO: 2022-10-07 07:32:03,806: Training the baseline model
INFO: 2022-10-07 07:32:03,813: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:32:13,789: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:32:13,789: Best score for XGBoost: 0.39417747252376456
INFO: 2022-10-07 07:32:13,796: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:32:13,796: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:32:13,796: Training the XGBoost model
INFO: 2022-10-07 07:32:14,250: XGB model Model saved
INFO: 2022-10-07 07:32:14,251: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:32:14,251: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:33:50,099: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:33:50,099: Lowest RMSE for random forest model: 0.6998904551257046
INFO: 2022-10-07 07:33:50,099: Training the random forest model
INFO: 2022-10-07 07:33:50,099: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:33:53,778: Random forest model saved
INFO: 2022-10-07 07:37:49,951: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:37:49,951: Splitting the data into train and test
INFO: 2022-10-07 07:37:50,023: Training the baseline model
INFO: 2022-10-07 07:37:50,023: Training the baseline model
INFO: 2022-10-07 07:37:50,025: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:37:59,144: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:37:59,144: Best score for XGBoost: 0.39526591647701326
INFO: 2022-10-07 07:37:59,150: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:37:59,150: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:37:59,151: Training the XGBoost model
INFO: 2022-10-07 07:37:59,750: XGB model Model saved
INFO: 2022-10-07 07:37:59,750: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:37:59,751: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:39:54,475: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-07 07:39:54,476: Lowest RMSE for random forest model: 0.7069632607641453
INFO: 2022-10-07 07:39:54,476: Training the random forest model
INFO: 2022-10-07 07:39:54,476: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 20, 'bootstrap': True}
INFO: 2022-10-07 07:39:59,660: Random forest model saved
INFO: 2022-10-07 07:41:22,287: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:41:22,287: Splitting the data into train and test
INFO: 2022-10-07 07:41:22,372: Training the baseline model
INFO: 2022-10-07 07:41:22,372: Training the baseline model
INFO: 2022-10-07 07:41:22,374: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:41:33,384: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:41:33,384: Best score for XGBoost: 0.4016591328181655
INFO: 2022-10-07 07:41:33,390: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:41:33,390: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:41:33,391: Training the XGBoost model
INFO: 2022-10-07 07:41:33,899: XGB model Model saved
INFO: 2022-10-07 07:41:33,899: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:41:33,899: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:43:05,396: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-07 07:43:05,396: Lowest RMSE for random forest model: 0.7052022642259864
INFO: 2022-10-07 07:43:05,397: Training the random forest model
INFO: 2022-10-07 07:43:05,397: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 70, 'bootstrap': True}
INFO: 2022-10-07 07:43:06,346: Random forest model saved
INFO: 2022-10-07 07:44:05,896: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:44:05,896: Splitting the data into train and test
INFO: 2022-10-07 07:44:06,007: Training the baseline model
INFO: 2022-10-07 07:44:06,007: Training the baseline model
INFO: 2022-10-07 07:44:06,008: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:44:16,785: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:44:16,785: Best score for XGBoost: 0.3974514840996458
INFO: 2022-10-07 07:44:16,791: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:44:16,792: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:44:16,792: Training the XGBoost model
INFO: 2022-10-07 07:44:17,311: XGB model Model saved
INFO: 2022-10-07 07:44:17,311: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:44:17,311: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:44:40,125: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:44:40,125: Lowest RMSE for random forest model: 0.700931935585323
INFO: 2022-10-07 07:44:40,126: Training the random forest model
INFO: 2022-10-07 07:44:40,126: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:44:40,949: Random forest model saved
INFO: 2022-10-07 07:45:05,076: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:45:05,076: Splitting the data into train and test
INFO: 2022-10-07 07:45:05,150: Training the baseline model
INFO: 2022-10-07 07:45:05,150: Training the baseline model
INFO: 2022-10-07 07:45:05,152: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:45:15,231: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:45:15,231: Best score for XGBoost: 0.3981536590332004
INFO: 2022-10-07 07:45:15,237: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:45:15,237: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:45:15,237: Training the XGBoost model
INFO: 2022-10-07 07:45:15,720: XGB model Model saved
INFO: 2022-10-07 07:45:15,721: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:45:15,721: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:45:35,853: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:45:35,853: Lowest RMSE for random forest model: 0.6988448498506555
INFO: 2022-10-07 07:45:35,854: Training the random forest model
INFO: 2022-10-07 07:45:35,854: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:45:36,530: Random forest model saved
INFO: 2022-10-07 07:52:53,352: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:52:53,352: Splitting the data into train and test
INFO: 2022-10-07 07:52:53,424: Training the baseline model
INFO: 2022-10-07 07:52:53,424: Training the baseline model
INFO: 2022-10-07 07:52:53,425: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:53:02,582: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:53:02,583: Best score for XGBoost: 0.3784521085126979
INFO: 2022-10-07 07:53:02,589: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:53:02,589: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:53:02,589: Training the XGBoost model
INFO: 2022-10-07 07:53:02,995: XGB model Model saved
INFO: 2022-10-07 07:53:02,996: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:53:02,996: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:53:21,949: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:53:21,949: Lowest RMSE for random forest model: 0.7010830990222078
INFO: 2022-10-07 07:53:21,950: Training the random forest model
INFO: 2022-10-07 07:53:21,950: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:53:22,578: Random forest model saved
INFO: 2022-10-07 07:56:00,606: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:56:00,606: Splitting the data into train and test
INFO: 2022-10-07 07:56:00,677: Training the baseline model
INFO: 2022-10-07 07:56:00,677: Training the baseline model
INFO: 2022-10-07 07:56:00,679: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:56:09,550: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:56:09,550: Best score for XGBoost: 0.3879346152719614
INFO: 2022-10-07 07:56:09,556: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:56:09,556: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:56:09,556: Training the XGBoost model
INFO: 2022-10-07 07:56:10,093: XGB model Model saved
INFO: 2022-10-07 07:56:10,094: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:56:10,094: Hyperparameter tuning for random forest model
INFO: 2022-10-07 07:56:36,649: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:56:36,649: Lowest RMSE for random forest model: 0.7013701036464891
INFO: 2022-10-07 07:56:36,649: Training the random forest model
INFO: 2022-10-07 07:56:36,650: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 07:56:37,373: Random forest model saved
INFO: 2022-10-07 07:59:45,736: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 07:59:45,736: Splitting the data into train and test
INFO: 2022-10-07 07:59:45,824: Training the baseline model
INFO: 2022-10-07 07:59:45,825: Training the baseline model
INFO: 2022-10-07 07:59:45,826: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 07:59:56,177: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 07:59:56,178: Best score for XGBoost: 0.4029118964297004
INFO: 2022-10-07 07:59:56,183: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 07:59:56,184: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 07:59:56,184: Training the XGBoost model
INFO: 2022-10-07 07:59:56,670: XGB model Model saved
INFO: 2022-10-07 07:59:56,670: Hyper parameter tuning for random forest
INFO: 2022-10-07 07:59:56,671: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:00:19,665: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:00:19,665: Lowest RMSE for random forest model: 0.7002522754049745
INFO: 2022-10-07 08:00:19,666: Training the random forest model
INFO: 2022-10-07 08:00:19,666: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:00:20,375: Random forest model saved
INFO: 2022-10-07 08:00:41,455: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:00:41,456: Splitting the data into train and test
INFO: 2022-10-07 08:00:41,548: Training the baseline model
INFO: 2022-10-07 08:00:41,548: Training the baseline model
INFO: 2022-10-07 08:00:41,550: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:00:53,594: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:00:53,594: Best score for XGBoost: 0.39137085227844765
INFO: 2022-10-07 08:00:53,603: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:00:53,604: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:00:53,604: Training the XGBoost model
INFO: 2022-10-07 08:00:54,393: XGB model Model saved
INFO: 2022-10-07 08:00:54,393: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:00:54,393: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:01:18,499: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:01:18,499: Lowest RMSE for random forest model: 0.6995964208554545
INFO: 2022-10-07 08:01:18,499: Training the random forest model
INFO: 2022-10-07 08:01:18,499: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:01:19,261: Random forest model saved
INFO: 2022-10-07 08:01:42,325: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:01:42,325: Splitting the data into train and test
INFO: 2022-10-07 08:01:42,345: Training the baseline model
INFO: 2022-10-07 08:01:42,345: Training the baseline model
INFO: 2022-10-07 08:01:42,346: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:01:55,764: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:01:55,764: Best score for XGBoost: 0.4127703886277837
INFO: 2022-10-07 08:01:55,770: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:01:55,770: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:01:55,770: Training the XGBoost model
INFO: 2022-10-07 08:01:56,277: XGB model Model saved
INFO: 2022-10-07 08:01:56,278: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:01:56,278: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:02:17,175: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:02:17,176: Lowest RMSE for random forest model: 0.6994617065339848
INFO: 2022-10-07 08:02:17,176: Training the random forest model
INFO: 2022-10-07 08:02:17,176: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:02:17,905: Random forest model saved
INFO: 2022-10-07 08:04:30,060: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:04:30,060: Splitting the data into train and test
INFO: 2022-10-07 08:04:30,148: Training the baseline model
INFO: 2022-10-07 08:04:30,149: Training the baseline model
INFO: 2022-10-07 08:04:30,150: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:04:40,450: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:04:40,450: Best score for XGBoost: 0.3809506524115175
INFO: 2022-10-07 08:04:40,456: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:04:40,457: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:04:40,457: Training the XGBoost model
INFO: 2022-10-07 08:04:40,987: XGB model Model saved
INFO: 2022-10-07 08:04:40,987: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:04:40,987: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:05:03,542: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:05:03,542: Lowest RMSE for random forest model: 0.6977100165122846
INFO: 2022-10-07 08:05:03,543: Training the random forest model
INFO: 2022-10-07 08:05:03,543: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:05:04,232: Random forest model saved
INFO: 2022-10-07 08:05:48,081: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:05:48,082: Splitting the data into train and test
INFO: 2022-10-07 08:05:48,161: Training the baseline model
INFO: 2022-10-07 08:05:48,161: Training the baseline model
INFO: 2022-10-07 08:05:48,163: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:05:59,644: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:05:59,644: Best score for XGBoost: 0.40184703954874673
INFO: 2022-10-07 08:05:59,651: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:05:59,651: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:05:59,651: Training the XGBoost model
INFO: 2022-10-07 08:06:00,356: XGB model Model saved
INFO: 2022-10-07 08:06:00,357: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:06:00,357: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:06:25,048: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:06:25,048: Lowest RMSE for random forest model: 0.6992307699648225
INFO: 2022-10-07 08:06:25,049: Training the random forest model
INFO: 2022-10-07 08:06:25,049: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:06:25,897: Random forest model saved
INFO: 2022-10-07 08:08:27,002: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:08:27,002: Splitting the data into train and test
INFO: 2022-10-07 08:08:27,074: Training the baseline model
INFO: 2022-10-07 08:08:27,074: Training the baseline model
INFO: 2022-10-07 08:08:27,076: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:08:36,391: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:08:36,391: Best score for XGBoost: 0.3932597300807276
INFO: 2022-10-07 08:08:36,397: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:08:36,397: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:08:36,397: Training the XGBoost model
INFO: 2022-10-07 08:08:36,834: XGB model Model saved
INFO: 2022-10-07 08:08:36,834: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:08:36,834: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:08:56,547: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:08:56,548: Lowest RMSE for random forest model: 0.6998710169265298
INFO: 2022-10-07 08:08:56,548: Training the random forest model
INFO: 2022-10-07 08:08:56,548: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:08:57,259: Random forest model saved
INFO: 2022-10-07 08:18:05,845: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:18:05,845: Splitting the data into train and test
INFO: 2022-10-07 08:18:05,916: Training the baseline model
INFO: 2022-10-07 08:18:05,916: Training the baseline model
INFO: 2022-10-07 08:18:05,922: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:18:15,121: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:18:15,122: Best score for XGBoost: 0.3815838604156327
INFO: 2022-10-07 08:18:15,128: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:18:15,128: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:18:15,128: Training the XGBoost model
INFO: 2022-10-07 08:18:15,578: XGB model Model saved
INFO: 2022-10-07 08:18:15,579: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:18:15,579: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:18:36,091: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:18:36,091: Lowest RMSE for random forest model: 0.69956691324819
INFO: 2022-10-07 08:18:36,091: Training the random forest model
INFO: 2022-10-07 08:18:36,091: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:18:36,797: Random forest model saved
INFO: 2022-10-07 08:19:27,672: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:19:27,672: Splitting the data into train and test
INFO: 2022-10-07 08:19:27,745: Training the baseline model
INFO: 2022-10-07 08:19:27,745: Training the baseline model
INFO: 2022-10-07 08:19:27,747: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:19:36,900: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:19:36,900: Best score for XGBoost: 0.39761256990557164
INFO: 2022-10-07 08:19:36,906: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:19:36,906: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:19:36,906: Training the XGBoost model
INFO: 2022-10-07 08:19:37,310: XGB model Model saved
INFO: 2022-10-07 08:19:37,311: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:19:37,311: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:19:57,125: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:19:57,125: Lowest RMSE for random forest model: 0.6988188702023046
INFO: 2022-10-07 08:19:57,125: Training the random forest model
INFO: 2022-10-07 08:19:57,126: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:19:57,771: Random forest model saved
INFO: 2022-10-07 08:21:23,318: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:21:23,318: Splitting the data into train and test
INFO: 2022-10-07 08:21:23,400: Training the baseline model
INFO: 2022-10-07 08:21:23,400: Training the baseline model
INFO: 2022-10-07 08:21:23,402: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:21:33,580: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:21:33,580: Best score for XGBoost: 0.38970609144442336
INFO: 2022-10-07 08:21:33,586: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:21:33,587: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:21:33,587: Training the XGBoost model
INFO: 2022-10-07 08:21:34,227: XGB model Model saved
INFO: 2022-10-07 08:21:34,227: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:21:34,228: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:22:10,685: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:22:10,685: Lowest RMSE for random forest model: 0.7000716554957914
INFO: 2022-10-07 08:22:10,685: Training the random forest model
INFO: 2022-10-07 08:22:10,685: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:22:11,397: Random forest model saved
INFO: 2022-10-07 08:42:55,469: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:42:55,469: Splitting the data into train and test
INFO: 2022-10-07 08:42:55,489: Training the baseline model
INFO: 2022-10-07 08:42:55,489: Training the baseline model
INFO: 2022-10-07 08:42:55,495: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:43:04,873: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:43:04,874: Best score for XGBoost: 0.38719210260002945
INFO: 2022-10-07 08:43:04,880: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:43:04,880: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:43:04,880: Training the XGBoost model
INFO: 2022-10-07 08:43:05,360: XGB model Model saved
INFO: 2022-10-07 08:43:05,361: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:43:05,361: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:43:26,831: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:43:26,832: Lowest RMSE for random forest model: 0.7018934582528897
INFO: 2022-10-07 08:43:26,832: Training the random forest model
INFO: 2022-10-07 08:43:26,832: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 2, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:43:27,526: Random forest model saved
INFO: 2022-10-07 08:44:12,677: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:44:12,677: Splitting the data into train and test
INFO: 2022-10-07 08:44:12,752: Training the baseline model
INFO: 2022-10-07 08:44:12,752: Training the baseline model
INFO: 2022-10-07 08:44:12,754: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:44:21,951: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:44:21,951: Best score for XGBoost: 0.3827805504435527
INFO: 2022-10-07 08:44:21,957: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=None, gamma=0.2, gpu_id=-1, grow_policy='depthwise',
             importance_type=None, interaction_constraints='',
             learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,
             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,
             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,
             num_parallel_tree=1, predictor='auto', random_state=42,
             reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:44:21,957: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:44:21,958: Training the XGBoost model
INFO: 2022-10-07 08:44:22,440: XGB model Model saved
INFO: 2022-10-07 08:44:22,440: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:44:22,440: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:46:30,969: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:46:30,969: Lowest RMSE for random forest model: 0.7078231266707596
INFO: 2022-10-07 08:46:30,970: Training the random forest model
INFO: 2022-10-07 08:46:30,970: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}
INFO: 2022-10-07 08:46:40,752: Random forest model saved
INFO: 2022-10-07 08:49:37,960: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:49:37,960: Splitting the data into train and test
INFO: 2022-10-07 08:49:38,039: Training the baseline model
INFO: 2022-10-07 08:49:38,039: Training the baseline model
INFO: 2022-10-07 08:49:38,041: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:49:48,008: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:49:48,008: Best score for XGBoost: 0.37879224016695046
INFO: 2022-10-07 08:49:48,015: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=['rmse'], gamma=0.2, gpu_id=-1,
             grow_policy='depthwise', importance_type=None,
             interaction_constraints='', learning_rate=0.3, max_bin=256,
             max_cat_to_onehot=4, max_delta_step=0, max_depth=4, max_leaves=0,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',
             random_state=42, reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:49:48,015: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:49:48,015: Training the XGBoost model
INFO: 2022-10-07 08:49:48,584: XGB model Model saved
INFO: 2022-10-07 08:49:48,584: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:49:48,585: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:51:50,162: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-07 08:51:50,163: Lowest RMSE for random forest model: 0.7037269971572665
INFO: 2022-10-07 08:51:50,170: Training the random forest model
INFO: 2022-10-07 08:51:50,170: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 50, 'bootstrap': True}
INFO: 2022-10-07 08:51:59,867: Random forest model saved
INFO: 2022-10-07 08:52:54,926: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:52:54,927: Splitting the data into train and test
INFO: 2022-10-07 08:52:55,053: Training the baseline model
INFO: 2022-10-07 08:52:55,054: Training the baseline model
INFO: 2022-10-07 08:52:55,057: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:53:05,766: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:53:05,767: Best score for XGBoost: -0.5490628008125493
INFO: 2022-10-07 08:53:05,774: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=['rmse'], gamma=0.2, gpu_id=-1,
             grow_policy='depthwise', importance_type=None,
             interaction_constraints='', learning_rate=0.3, max_bin=256,
             max_cat_to_onehot=4, max_delta_step=0, max_depth=4, max_leaves=0,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',
             random_state=42, reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:53:05,774: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:53:05,774: Training the XGBoost model
INFO: 2022-10-07 08:53:06,403: XGB model Model saved
INFO: 2022-10-07 08:53:06,404: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:53:06,404: Hyperparameter tuning for random forest model
INFO: 2022-10-07 08:54:06,759: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-07 08:54:06,760: Lowest RMSE for random forest model: 0.7050382683743902
INFO: 2022-10-07 08:54:06,760: Training the random forest model
INFO: 2022-10-07 08:54:06,760: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 40, 'bootstrap': True}
INFO: 2022-10-07 08:54:08,539: Random forest model saved
INFO: 2022-10-07 08:58:30,115: Reading the final data from the data/processed folder and training the model
INFO: 2022-10-07 08:58:30,116: Splitting the data into train and test
INFO: 2022-10-07 08:58:30,234: Training the baseline model
INFO: 2022-10-07 08:58:30,235: Training the baseline model
INFO: 2022-10-07 08:58:30,240: Hyperparameter tuning of XGBoost model
INFO: 2022-10-07 08:58:49,928: Best parameters for XGBoost: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.3, 'gamma': 0.2, 'colsample_bytree': 0.7}
INFO: 2022-10-07 08:58:49,928: Best score for XGBoost: -0.5590524361596221
INFO: 2022-10-07 08:58:49,936: Best estimator for XGBoost: XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,
             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
             early_stopping_rounds=None, enable_categorical=False,
             eval_metric=['rmse'], gamma=0.2, gpu_id=-1,
             grow_policy='depthwise', importance_type=None,
             interaction_constraints='', learning_rate=0.3, max_bin=256,
             max_cat_to_onehot=4, max_delta_step=0, max_depth=4, max_leaves=0,
             min_child_weight=1, missing=nan, monotone_constraints='()',
             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',
             random_state=42, reg_alpha=0, reg_lambda=1, ...)
INFO: 2022-10-07 08:58:49,936: Training the XGBoost model with best hyperparameters
INFO: 2022-10-07 08:58:49,936: Training the XGBoost model
INFO: 2022-10-07 08:58:50,822: XGB model Model saved
INFO: 2022-10-07 08:58:50,822: Hyper parameter tuning for random forest
INFO: 2022-10-07 08:58:50,822: Hyperparameter tuning for random forest model
INFO: 2022-10-07 09:00:09,521: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-07 09:00:09,522: Lowest RMSE for random forest model: 0.704917498846308
INFO: 2022-10-07 09:00:09,523: Training the random forest model
INFO: 2022-10-07 09:00:09,523: Best parameters for random forest model: {'random_state': 2022, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 2, 'max_depth': 80, 'bootstrap': True}
INFO: 2022-10-07 09:00:10,952: Random forest model saved
